[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "mp01",
    "section": "",
    "text": "Code\n# (Starter code — unchanged)\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "mp01.html#acquire-data",
    "href": "mp01.html#acquire-data",
    "title": "mp01",
    "section": "",
    "text": "Code\n# (Starter code — unchanged)\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n    dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                  destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n    download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                  destfile=COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "mp01.html#data-import-and-preparation",
    "href": "mp01.html#data-import-and-preparation",
    "title": "mp01",
    "section": "Data Import and Preparation",
    "text": "Data Import and Preparation\n\n\nCode\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\n\n\n\n\nCode\nGLOBAL_TOP_10  &lt;- read_tsv(GLOBAL_TOP_10_FILENAME, show_col_types = FALSE)\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME, show_col_types = FALSE)\n\nstr(GLOBAL_TOP_10)\n\n\nspc_tbl_ [8,880 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8880], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:8880] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8880] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8880] \"KPop Demon Hunters\" \"Ruth & Boaz\" \"The Wrong Paris\" \"Man on Fire\" ...\n $ season_title              : chr [1:8880] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ weekly_hours_viewed       : num [1:8880] 32200000 15900000 13500000 15700000 11200000 8400000 6800000 6200000 4900000 8400000 ...\n $ runtime                   : num [1:8880] 1.67 1.55 1.78 2.43 1.83 ...\n $ weekly_views              : num [1:8880] 19300000 10300000 7600000 6500000 6100000 4900000 3600000 3200000 3200000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8880] 15 1 3 5 2 1 1 1 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\ndplyr::glimpse(GLOBAL_TOP_10)\n\n\nRows: 8,880\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"KPop Demon Hunters\", \"Ruth & Boaz\", \"The W…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ weekly_hours_viewed        &lt;dbl&gt; 32200000, 15900000, 13500000, 15700000, 112…\n$ runtime                    &lt;dbl&gt; 1.6667, 1.5500, 1.7833, 2.4333, 1.8333, 1.7…\n$ weekly_views               &lt;dbl&gt; 19300000, 10300000, 7600000, 6500000, 61000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 15, 1, 3, 5, 2, 1, 1, 1, 1, 3, 2, 1, 1, 2, …\n\n\nCode\nstr(COUNTRY_TOP_10)\n\n\nspc_tbl_ [413,620 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ country_name              : chr [1:413620] \"Argentina\" \"Argentina\" \"Argentina\" \"Argentina\" ...\n $ country_iso2              : chr [1:413620] \"AR\" \"AR\" \"AR\" \"AR\" ...\n $ week                      : Date[1:413620], format: \"2025-09-28\" \"2025-09-28\" ...\n $ category                  : chr [1:413620] \"Films\" \"Films\" \"Films\" \"Films\" ...\n $ weekly_rank               : num [1:413620] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:413620] \"Sonic the Hedgehog 3\" \"KPop Demon Hunters\" \"French Lover\" \"She Said Maybe\" ...\n $ season_title              : chr [1:413620] \"N/A\" \"N/A\" \"N/A\" \"N/A\" ...\n $ cumulative_weeks_in_top_10: num [1:413620] 2 15 1 2 1 1 2 5 1 2 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   country_name = col_character(),\n  ..   country_iso2 = col_character(),\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nCode\ndplyr::glimpse(COUNTRY_TOP_10)\n\n\nRows: 413,620\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-28, 2025-09-28, 2025-09-28, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"Sonic the Hedgehog 3\", \"KPop Demon Hunters…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 15, 1, 2, 1, 1, 2, 5, 1, 2, 2, 1, 1, 1, …\n\n\nCode\nGLOBAL_TOP_10  &lt;- GLOBAL_TOP_10  |&gt; mutate(week = as.Date(week))\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 |&gt; mutate(week = as.Date(week))\n\nsummary(GLOBAL_TOP_10$week)\n\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2021-07-04\" \"2022-07-24\" \"2023-08-16\" \"2023-08-16\" \"2024-09-08\" \"2025-09-28\" \n\n\nCode\nsummary(COUNTRY_TOP_10$week)\n\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2021-07-04\" \"2022-07-24\" \"2023-08-13\" \"2023-08-15\" \"2024-09-08\" \"2025-09-28\""
  },
  {
    "objectID": "mp01.html#task-2---data-cleaning",
    "href": "mp01.html#task-2---data-cleaning",
    "title": "mp01",
    "section": "Task 2 - Data Cleaning",
    "text": "Task 2 - Data Cleaning\n\n\nCode\n# Convert literal \"N/A\" strings in season_title to real NA\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\n  mutate(season_title = if_else(season_title == \"N/A\",\n                                NA_character_,\n                                season_title))\n\n# quick confirmation\nGLOBAL_TOP_10 |&gt;\n  summarise(\n    n_rows = n(),\n    na_season_title = sum(is.na(season_title))\n  )\n\n\n# A tibble: 1 × 2\n  n_rows na_season_title\n   &lt;int&gt;           &lt;int&gt;\n1   8880            4562"
  },
  {
    "objectID": "mp01.html#task-3---data-import-tweak",
    "href": "mp01.html#task-3---data-import-tweak",
    "title": "mp01",
    "section": "Task 3 - Data Import Tweak",
    "text": "Task 3 - Data Import Tweak\n\n\nCode\nCOUNTRY_TOP_10 &lt;- read_tsv(\n  COUNTRY_TOP_10_FILENAME,\n  show_col_types = FALSE,\n  na = c(\"N/A\")   # treat the literal string \"N/A\" as missing\n)\n\n# quick confirmation\nCOUNTRY_TOP_10 |&gt;\n  summarise(\n    n_rows = n(),\n    na_season_title = sum(is.na(season_title))\n  )\n\n\n# A tibble: 1 × 2\n  n_rows na_season_title\n   &lt;int&gt;           &lt;int&gt;\n1 413620          210931"
  },
  {
    "objectID": "mp01.html#dark-mode-fix",
    "href": "mp01.html#dark-mode-fix",
    "title": "mp01",
    "section": "Dark Mode Fix",
    "text": "Dark Mode Fix"
  },
  {
    "objectID": "mp01.html#dark-mode-css-fix",
    "href": "mp01.html#dark-mode-css-fix",
    "title": "mp01",
    "section": "Dark Mode CSS Fix",
    "text": "Dark Mode CSS Fix\n\n\nCode\n:root[data-bs-theme=\"dark\"] table.dataTable,\n:root[data-bs-theme=\"dark\"] .dataTables_wrapper .dataTables_info,\n:root[data-bs-theme=\"dark\"] .dataTables_wrapper .dataTables_length label,\n:root[data-bs-theme=\"dark\"] .dataTables_wrapper .dataTables_filter label {\n  color: #e6e6e6 !important;\n}\n:root[data-bs-theme=\"dark\"] table.dataTable thead th {\n  color: #f1f5f9 !important;\n}"
  },
  {
    "objectID": "mp01.html#initial-data-exploration",
    "href": "mp01.html#initial-data-exploration",
    "title": "mp01",
    "section": "Initial Data Exploration",
    "text": "Initial Data Exploration\n\n\nCode\n# Install once if needed\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\n# Show a small interactive preview of the global data\ntbl &lt;- GLOBAL_TOP_10 |&gt;\n  head(n = 20) |&gt;\n  datatable_dark()\n\n# (optional) also preview the country-level data\ntbl_country &lt;- COUNTRY_TOP_10 |&gt;\n  head(n = 20) |&gt;\n  datatable_dark()"
  },
  {
    "objectID": "mp01.html#cleaned-preview-table",
    "href": "mp01.html#cleaned-preview-table",
    "title": "mp01",
    "section": "Cleaned Preview Table",
    "text": "Cleaned Preview Table\n\n\nCode\n# header formatting helper\nif (!require(\"stringr\")) install.packages(\"stringr\")\nlibrary(stringr)\n\nformat_titles &lt;- function(df){\n  colnames(df) &lt;- colnames(df) |&gt;\n    str_replace_all(\"_\", \" \") |&gt;\n    str_to_title()\n  df\n}\n\n# Build the pretty table from GLOBAL_TOP_10\ntbl &lt;- GLOBAL_TOP_10 |&gt;\n  format_titles() |&gt;\n  head(n = 20) |&gt;\n  datatable_dark()\n\n# Round / format the numeric columns if present\nnum_cols &lt;- intersect(c(\"Weekly Hours Viewed\", \"Weekly Views\"), colnames(tbl$x$data))\nDT::formatRound(tbl, columns = num_cols, digits = 0)"
  },
  {
    "objectID": "mp01.html#film-only-preview",
    "href": "mp01.html#film-only-preview",
    "title": "mp01",
    "section": "Film-only Preview",
    "text": "Film-only Preview\n\n\nCode\n# Build the table while dropping `season_title`\ntbl2 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::select(-season_title) |&gt;\n  format_titles() |&gt;\n  head(n = 20) |&gt;\n  datatable_dark()\n\n# Format big numbers if present\nnum_cols &lt;- intersect(c(\"Weekly Hours Viewed\", \"Weekly Views\"), colnames(tbl2$x$data))\nDT::formatRound(tbl2, columns = num_cols, digits = 0)"
  },
  {
    "objectID": "mp01.html#runtime-in-minutes",
    "href": "mp01.html#runtime-in-minutes",
    "title": "mp01",
    "section": "Runtime in Minutes",
    "text": "Runtime in Minutes\n\n\nCode\n# If your dataset has a 'runtime' (in hours), convert to minutes and show a tidy preview\nif (\"runtime\" %in% names(GLOBAL_TOP_10)) {\n  tbl3 &lt;- GLOBAL_TOP_10 |&gt;\n    dplyr::mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    dplyr::select(-season_title, -runtime) |&gt;\n    format_titles() |&gt;\n    head(n = 20) |&gt;\n    datatable_dark()\n\n  # Format big numeric columns if present\n  num_cols &lt;- intersect(c(\"Weekly Hours Viewed\", \"Weekly Views\"), colnames(tbl3$x$data))\n  DT::formatRound(tbl3, columns = num_cols, digits = 0)\n} else {\n  message(\"No 'runtime' column found in GLOBAL_TOP_10; skipping this view.\")\n}"
  },
  {
    "objectID": "mp01.html#question-setup",
    "href": "mp01.html#question-setup",
    "title": "mp01",
    "section": "Question Setup",
    "text": "Question Setup\n\n\nCode\n# Figure out which column holds the country name\ncountry_col &lt;- intersect(names(COUNTRY_TOP_10), c(\"country\", \"country_name\"))[1]\nstopifnot(!is.na(country_col))  # fail early if neither exists"
  },
  {
    "objectID": "mp01.html#q1-countries-with-top-10-data-proxy-for-operates-in",
    "href": "mp01.html#q1-countries-with-top-10-data-proxy-for-operates-in",
    "title": "mp01",
    "section": "Q1 — Countries with Top-10 data (proxy for “operates in”)",
    "text": "Q1 — Countries with Top-10 data (proxy for “operates in”)\n\n\nCode\nn_countries &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::summarise(n_countries = dplyr::n_distinct(.data[[country_col]])) |&gt;\n  dplyr::pull(n_countries)\nn_countries\n\n\n[1] 94"
  },
  {
    "objectID": "mp01.html#q2-non-english-films-most-cumulative-weeks-in-global-top-10",
    "href": "mp01.html#q2-non-english-films-most-cumulative-weeks-in-global-top-10",
    "title": "mp01",
    "section": "Q2 — Non-English Films: most cumulative weeks in global Top 10",
    "text": "Q2 — Non-English Films: most cumulative weeks in global Top 10\n\n\nCode\nq2 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::filter(category == \"Films (Non-English)\") |&gt;\n  dplyr::group_by(show_title) |&gt;\n  dplyr::summarise(weeks = dplyr::n_distinct(week), .groups = \"drop\") |&gt;\n  dplyr::arrange(dplyr::desc(weeks))\nq2 |&gt; dplyr::slice_max(weeks, n = 1)\n\n\n# A tibble: 1 × 2\n  show_title                     weeks\n  &lt;chr&gt;                          &lt;int&gt;\n1 All Quiet on the Western Front    23"
  },
  {
    "objectID": "mp01.html#q3-longest-film-any-language-to-appear-in-global-top-10",
    "href": "mp01.html#q3-longest-film-any-language-to-appear-in-global-top-10",
    "title": "mp01",
    "section": "Q3 — Longest film (any language) to appear in global Top 10",
    "text": "Q3 — Longest film (any language) to appear in global Top 10\n\n\nCode\nq3 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::filter(stringr::str_starts(category, \"Films\"),\n                !is.na(runtime)) |&gt;\n  dplyr::mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  dplyr::arrange(dplyr::desc(runtime_minutes))\nq3 |&gt; dplyr::slice_head(n = 1) |&gt;\n  dplyr::select(show_title, runtime_minutes)\n\n\n# A tibble: 1 × 2\n  show_title                            runtime_minutes\n  &lt;chr&gt;                                           &lt;dbl&gt;\n1 Pushpa 2: The Rule (Reloaded Version)             224"
  },
  {
    "objectID": "mp01.html#q4-most-total-hours-viewed-by-category-program-show_title",
    "href": "mp01.html#q4-most-total-hours-viewed-by-category-program-show_title",
    "title": "mp01",
    "section": "Q4 — Most total hours viewed by category (program = show_title)",
    "text": "Q4 — Most total hours viewed by category (program = show_title)\n\n\nCode\nq4 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::group_by(category, show_title) |&gt;\n  dplyr::summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE),\n                   .groups = \"drop_last\") |&gt;\n  dplyr::slice_max(total_hours, n = 1, with_ties = FALSE) |&gt;\n  dplyr::ungroup() |&gt;\n  dplyr::arrange(category)\nq4\n\n\n# A tibble: 4 × 3\n  category            show_title          total_hours\n  &lt;chr&gt;               &lt;chr&gt;                     &lt;dbl&gt;\n1 Films (English)     KPop Demon Hunters    591300000\n2 Films (Non-English) Society of the Snow   235900000\n3 TV (English)        Stranger Things      2967980000\n4 TV (Non-English)    Squid Game           5048300000"
  },
  {
    "objectID": "mp01.html#q5-longest-top-10-run-for-a-tv-show-in-any-one-country",
    "href": "mp01.html#q5-longest-top-10-run-for-a-tv-show-in-any-one-country",
    "title": "mp01",
    "section": "Q5 — Longest Top-10 run for a TV show in any one country",
    "text": "Q5 — Longest Top-10 run for a TV show in any one country\n\n\nCode\nq5 &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::filter(stringr::str_starts(category, \"TV\")) |&gt;\n  dplyr::group_by(.data[[country_col]], show_title) |&gt;\n  dplyr::summarise(weeks = dplyr::n_distinct(week), .groups = \"drop\") |&gt;\n  dplyr::arrange(dplyr::desc(weeks))\nq5 |&gt; dplyr::slice_head(n = 1)\n\n\n# A tibble: 1 × 3\n  country_name show_title  weeks\n  &lt;chr&gt;        &lt;chr&gt;       &lt;int&gt;\n1 Pakistan     Money Heist   128"
  },
  {
    "objectID": "mp01.html#q6-country-with-200-weeks-of-history-and-last-week-available",
    "href": "mp01.html#q6-country-with-200-weeks-of-history-and-last-week-available",
    "title": "mp01",
    "section": "Q6 — Country with < 200 weeks of history and last week available",
    "text": "Q6 — Country with &lt; 200 weeks of history and last week available\n\n\nCode\nq6_country &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::summarise(n_weeks = dplyr::n_distinct(week), .by = .data[[country_col]]) |&gt;\n  dplyr::arrange(n_weeks) |&gt;\n  dplyr::slice_head(n = 1)\n\n\nWarning: Use of .data in tidyselect expressions was deprecated in tidyselect 1.2.0.\nℹ Please use `all_of(var)` (or `any_of(var)`) instead of `.data[[var]]`\n\n\nCode\nq6_last_week &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::filter(.data[[country_col]] == q6_country[[country_col]][1]) |&gt;\n  dplyr::summarise(last_week = max(week, na.rm = TRUE))\n\ndplyr::bind_cols(q6_country, q6_last_week)\n\n\n# A tibble: 1 × 3\n  country_name n_weeks last_week \n  &lt;chr&gt;          &lt;int&gt; &lt;date&gt;    \n1 Russia            35 2022-02-27"
  },
  {
    "objectID": "mp01.html#q7-squid-game-total-hours-across-all-seasons-global",
    "href": "mp01.html#q7-squid-game-total-hours-across-all-seasons-global",
    "title": "mp01",
    "section": "Q7 — Squid Game: total hours across all seasons (global)",
    "text": "Q7 — Squid Game: total hours across all seasons (global)\n\n\nCode\nq7 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::filter(stringr::str_starts(category, \"TV\"),\n                show_title == \"Squid Game\") |&gt;\n  dplyr::summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\nq7\n\n\n# A tibble: 1 × 1\n  total_hours\n        &lt;dbl&gt;\n1  5048300000"
  },
  {
    "objectID": "mp01.html#q8-red-notice-approx.-views-in-2021",
    "href": "mp01.html#q8-red-notice-approx.-views-in-2021",
    "title": "mp01",
    "section": "Q8 — Red Notice approx. views in 2021",
    "text": "Q8 — Red Notice approx. views in 2021\n\n\nCode\nq8 &lt;- GLOBAL_TOP_10 |&gt;\n  dplyr::filter(show_title == \"Red Notice\",\n                lubridate::year(week) == 2021) |&gt;\n  dplyr::mutate(runtime_minutes = round(60 * runtime)) |&gt;\n  dplyr::summarise(\n    total_hours_2021 = sum(weekly_hours_viewed, na.rm = TRUE),\n    runtime_minutes  = dplyr::first(runtime_minutes)\n  ) |&gt;\n  dplyr::mutate(approx_views_2021 = (total_hours_2021 * 60) / runtime_minutes)\nq8\n\n\n# A tibble: 1 × 3\n  total_hours_2021 runtime_minutes approx_views_2021\n             &lt;dbl&gt;           &lt;dbl&gt;             &lt;dbl&gt;\n1        396740000              NA                NA"
  },
  {
    "objectID": "mp01.html#q9-films-that-later-reached-1-in-us-but-did-not-debut-at-1",
    "href": "mp01.html#q9-films-that-later-reached-1-in-us-but-did-not-debut-at-1",
    "title": "mp01",
    "section": "Q9 — Films that later reached #1 in US but did not debut at #1",
    "text": "Q9 — Films that later reached #1 in US but did not debut at #1\n\n\nCode\nus_name &lt;- \"United States\"\n\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::filter(.data[[country_col]] == us_name,\n                stringr::str_starts(category, \"Films\")) |&gt;\n  dplyr::arrange(show_title, week)\n\ndebut_us &lt;- us_films |&gt;\n  dplyr::slice_min(week, by = show_title, with_ties = FALSE) |&gt;\n  dplyr::select(show_title, debut_rank = weekly_rank)\n\nmin_us &lt;- us_films |&gt;\n  dplyr::summarise(min_rank = min(weekly_rank, na.rm = TRUE), .by = show_title)\n\nq9 &lt;- debut_us |&gt;\n  dplyr::inner_join(min_us, by = \"show_title\") |&gt;\n  dplyr::filter(debut_rank != 1, min_rank == 1)\n\nq9_most_recent &lt;- us_films |&gt;\n  dplyr::filter(show_title %in% q9$show_title, weekly_rank == 1) |&gt;\n  dplyr::slice_max(week, by = show_title, with_ties = FALSE) |&gt;\n  dplyr::slice_max(week, n = 1)\n\nlist(\n  count = nrow(q9),\n  examples = q9 |&gt; dplyr::arrange(show_title),\n  most_recent = q9_most_recent |&gt; dplyr::select(show_title, week)\n)\n\n\n$count\n[1] 45\n\n$examples\n# A tibble: 45 × 3\n   show_title      debut_rank min_rank\n   &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n 1 Aftermath                4        1\n 2 American Made            9        1\n 3 Blood Red Sky            5        1\n 4 Bullet Train             2        1\n 5 Day Shift                2        1\n 6 Despicable Me 2          2        1\n 7 Despicable Me 4          2        1\n 8 Dog Gone                 4        1\n 9 Don't Move               3        1\n10 End of the Road          2        1\n# ℹ 35 more rows\n\n$most_recent\n# A tibble: 1 × 2\n  show_title         week      \n  &lt;chr&gt;              &lt;date&gt;    \n1 KPop Demon Hunters 2025-09-28"
  },
  {
    "objectID": "mp01.html#q10-tv-showseason-with-widest-top-10-debut-most-countries",
    "href": "mp01.html#q10-tv-showseason-with-widest-top-10-debut-most-countries",
    "title": "mp01",
    "section": "Q10 — TV show/season with widest Top-10 debut (most countries)",
    "text": "Q10 — TV show/season with widest Top-10 debut (most countries)\n\n\nCode\n# Define each title's debut week per country\ndebut_by_country &lt;- COUNTRY_TOP_10 |&gt;\n  dplyr::filter(stringr::str_starts(category, \"TV\")) |&gt;\n  dplyr::group_by(.data[[country_col]], show_title, season_title) |&gt;\n  dplyr::summarise(debut_week = min(week), .groups = \"drop\")\n\nq10 &lt;- debut_by_country |&gt;\n  dplyr::count(show_title, season_title, name = \"n_countries\") |&gt;\n  dplyr::arrange(dplyr::desc(n_countries))\n\nq10 |&gt; dplyr::slice_head(n = 1)\n\n\n# A tibble: 1 × 3\n  show_title         season_title                 n_countries\n  &lt;chr&gt;              &lt;chr&gt;                              &lt;int&gt;\n1 All of Us Are Dead All of Us Are Dead: Season 1          94"
  },
  {
    "objectID": "mp01.html#column-helpers",
    "href": "mp01.html#column-helpers",
    "title": "mp01",
    "section": "Column helpers",
    "text": "Column helpers\n\n\nCode\n## Column name check (just to see exact spellings)\nnames(GLOBAL_TOP_10)\n\n\n[1] \"week\"                       \"category\"                  \n[3] \"weekly_rank\"                \"show_title\"                \n[5] \"season_title\"               \"weekly_hours_viewed\"       \n[7] \"runtime\"                    \"weekly_views\"              \n[9] \"cumulative_weeks_in_top_10\"\n\n\nCode\nnames(COUNTRY_TOP_10)\n\n\n[1] \"country_name\"               \"country_iso2\"              \n[3] \"week\"                       \"category\"                  \n[5] \"weekly_rank\"                \"show_title\"                \n[7] \"season_title\"               \"cumulative_weeks_in_top_10\"\n\n\n\n\nCode\nhours_col_g &lt;- \"weekly_hours_viewed\"   # GLOBAL_TOP_10 hours column\ncountry_col &lt;- \"country_name\"          # COUNTRY_TOP_10 country column\n\n# quick sanity\nstopifnot(hours_col_g %in% names(GLOBAL_TOP_10))\nstopifnot(country_col  %in% names(COUNTRY_TOP_10))"
  },
  {
    "objectID": "mp01.html#stranger-things",
    "href": "mp01.html#stranger-things",
    "title": "mp01",
    "section": "Stranger Things",
    "text": "Stranger Things\n\n\nCode\nlibrary(dplyr); library(ggplot2); library(scales)\n\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nCode\nst_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(grepl(\"^TV\", category), show_title == \"Stranger Things\")\n\nst_by_season &lt;- st_global %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    weeks = n_distinct(week),\n    hours = sum(.data[[hours_col_g]], na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(season_title)\n\nst_total_hours   &lt;- sum(st_by_season$hours, na.rm = TRUE)\nst_longest_season &lt;- st_by_season %&gt;% slice_max(weeks, n = 1)\n\nst_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(grepl(\"^TV\", category), show_title == \"Stranger Things\") %&gt;%\n  summarise(n_countries = n_distinct(.data[[country_col]])) %&gt;%\n  pull(n_countries)\n\n# Charts\nggplot(st_by_season, aes(x = season_title, y = hours)) +\n  geom_col() +\n  labs(title = \"Stranger Things — Total Global Hours by Season\",\n       x = \"Season\", y = \"Hours\") +\n  scale_y_continuous(labels = label_comma())\n\n\n\n\n\n\n\n\n\nCode\nst_weekly &lt;- st_global %&gt;%\n  group_by(week) %&gt;%\n  summarise(hours = sum(.data[[hours_col_g]], na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(week) %&gt;%\n  mutate(cum_hours = cumsum(hours))\n\nggplot(st_weekly, aes(x = week, y = cum_hours)) +\n  geom_line() +\n  labs(title = \"Stranger Things — Cumulative Global Hours\",\n       x = \"Week\", y = \"Cumulative Hours\") +\n  scale_y_continuous(labels = label_comma())"
  },
  {
    "objectID": "mp01.html#india-commercial-success",
    "href": "mp01.html#india-commercial-success",
    "title": "mp01",
    "section": "India Commercial Success",
    "text": "India Commercial Success\n\n\nCode\n# packages\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n})\nindia &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(.data[[country_col]] == \"India\") %&gt;%\n  arrange(week)\n\n## 1) #1 winners: which titles spent the most weeks at #1?\nindia_no1 &lt;- india %&gt;%\n  group_by(week) %&gt;%\n  slice_min(weekly_rank, with_ties = FALSE) %&gt;%    # one #1 per week\n  ungroup()\n\nno1_by_title &lt;- india_no1 %&gt;%\n  count(show_title, name = \"weeks_at_no1\") %&gt;%\n  arrange(desc(weeks_at_no1)) %&gt;%\n  slice_head(n = 12)\n\nggplot(no1_by_title,\n       aes(x = weeks_at_no1, y = fct_reorder(show_title, weeks_at_no1))) +\n  geom_col() +\n  labs(title = \"India — Weeks at #1 by Title\",\n       x = \"Weeks at #1\", y = \"Title\")\n\n\n\n\n\n\n\n\n\nCode\n## 2) Longest #1 streaks (consecutive weeks at #1)\n# run-length encoding across weeks\nstreaks &lt;- india_no1 %&gt;%\n  mutate(change = show_title != dplyr::lag(show_title, default = first(show_title)),\n         streak_id = cumsum(change)) %&gt;%\n  group_by(streak_id, show_title) %&gt;%\n  summarise(\n    streak_weeks = n(),\n    start = min(week),\n    end   = max(week),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(streak_weeks)) %&gt;%\n  slice_head(n = 10)\n\nstreaks  # small table to quote in the PR\n\n\n# A tibble: 10 × 5\n   streak_id show_title          streak_weeks start      end       \n       &lt;int&gt; &lt;chr&gt;                      &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n 1         0 Haseen Dillruba                4 2021-07-04 2021-07-25\n 2         7 Thalaivii                      4 2021-09-26 2021-10-17\n 3       117 Lucky Baskhar                  4 2024-12-01 2024-12-22\n 4        13 Sooryavanshi                   3 2021-12-05 2021-12-19\n 5        22 Dasvi                          3 2022-04-10 2022-04-24\n 6        25 RRR (Hindi)                    3 2022-05-22 2022-06-05\n 7        69 Lust Stories 2                 3 2023-07-02 2023-07-16\n 8        81 Jawan: Extended Cut            3 2023-11-05 2023-11-19\n 9        92 Dunki                          3 2024-02-18 2024-03-03\n10        95 Fighter                        3 2024-03-24 2024-04-07\n\n\nCode\nggplot(streaks,\n       aes(x = start, xend = end, y = fct_reorder(show_title, streak_weeks), yend = show_title)) +\n  geom_segment(linewidth = 4) +\n  labs(title = \"India — Longest Consecutive #1 Streaks\",\n       x = \"Calendar time\", y = \"Title\")\n\n\n\n\n\n\n\n\n\nCode\n## 3) Rank heatmap for the most persistent titles (by weeks in Top-10)\ntop_titles_by_weeks &lt;- india %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(weeks_in_top10 = n_distinct(week), .groups = \"drop\") %&gt;%\n  arrange(desc(weeks_in_top10)) %&gt;%\n  slice_head(n = 8) %&gt;%\n  pull(show_title)\n\nheat &lt;- india %&gt;%\n  filter(show_title %in% top_titles_by_weeks)\n\nggplot(heat, aes(x = week, y = fct_rev(fct_reorder(show_title, as.numeric(factor(show_title)))),\n                 fill = 11 - weekly_rank)) +\n  geom_tile() +\n  scale_fill_continuous(name = \"Rank (bright = #1)\", breaks = c(10, 6, 2),\n                        labels = c(\"Rank 1\", \"Rank 5\", \"Rank 9\")) +\n  scale_y_discrete(name = \"Title\") +\n  labs(title = \"India — Weekly Rank Heatmap for Top Persistent Titles\", x = \"Week\") +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "mp01.html#open-topic---squid-games",
    "href": "mp01.html#open-topic---squid-games",
    "title": "mp01",
    "section": "Open Topic - Squid Games",
    "text": "Open Topic - Squid Games\n\n\nCode\nsuppressPackageStartupMessages({\n  library(dplyr); library(ggplot2); library(forcats); library(scales); library(stringr)\n})\n\ntitle_of_interest &lt;- \"Squid Game\"\n\n## Data slices\nsg_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(grepl(\"^TV\", category), show_title == title_of_interest) %&gt;%\n  arrange(week)\n\nsg_country &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(show_title == title_of_interest)\n\n## Headline facts (for inline text)\nsg_total_hours &lt;- sg_global %&gt;%\n  summarise(total = sum(.data[[hours_col_g]], na.rm = TRUE)) %&gt;% pull(total)\n\nsg_weeks_global &lt;- n_distinct(sg_global$week)\n\nsg_country_reach &lt;- sg_country %&gt;%\n  summarise(n_countries = n_distinct(.data[[country_col]])) %&gt;% pull(n_countries)\n\n# By season (hours + weeks) — handy to cite S1/S2/S3 breakdowns if present\nsg_by_season &lt;- sg_global %&gt;%\n  mutate(season_title = if_else(is.na(season_title), \"Unknown/All\", season_title)) %&gt;%\n  group_by(season_title) %&gt;%\n  summarise(\n    weeks = n_distinct(week),\n    hours = sum(.data[[hours_col_g]], na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;% arrange(season_title)\n\nsg_by_season\n\n\n# A tibble: 3 × 3\n  season_title         weeks      hours\n  &lt;chr&gt;                &lt;int&gt;      &lt;dbl&gt;\n1 Squid Game: Season 1    32 2729400000\n2 Squid Game: Season 2    14 1445500000\n3 Squid Game: Season 3     9  873400000\n\n\nCode\n## Visual 1: Top countries by *weeks in Top-10*\nsg_top_countries &lt;- sg_country %&gt;%\n  group_by(.data[[country_col]]) %&gt;%\n  summarise(weeks = n_distinct(week), .groups = \"drop\") %&gt;%\n  slice_max(weeks, n = 12) %&gt;% arrange(weeks)\n\nggplot(sg_top_countries,\n       aes(x = weeks, y = reorder(.data[[country_col]], weeks))) +\n  geom_col() +\n  labs(\n    title = \"Squid Game — Top Countries by Weeks in Top-10\",\n    x = \"Weeks in Top-10\", y = \"Country\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# top 3 country names for inline copy\nsg_top3 &lt;- sg_top_countries %&gt;%\n  arrange(desc(weeks)) %&gt;% slice_head(n = 3) %&gt;% pull(.data[[country_col]])\n\n## Visual 2: Cumulative global hours over time\nsg_weekly_hours &lt;- sg_global %&gt;%\n  group_by(week) %&gt;%\n  summarise(hours = sum(.data[[hours_col_g]], na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(week) %&gt;%\n  mutate(cum_hours = cumsum(hours))\n\nggplot(sg_weekly_hours, aes(x = week, y = cum_hours)) +\n  geom_line() +\n  labs(\n    title = \"Squid Game — Cumulative Global Hours\",\n    x = \"Week\", y = \"Cumulative Hours\"\n  ) +\n  scale_y_continuous(labels = label_comma())\n\n\n\n\n\n\n\n\n\nCode\n## Visual 3: Longest continuous Top-10 run by country (streak)\n# (any rank 1–10)\nsg_streaks &lt;- sg_country %&gt;%\n  arrange(.data[[country_col]], week) %&gt;%\n  group_by(.data[[country_col]]) %&gt;%\n  mutate(\n    gap = as.integer(week) - as.integer(lag(week)),\n    new_block = if_else(is.na(gap) | gap &gt; 7, 1L, 0L),        # gaps &gt; 1 week break streak\n    block_id = cumsum(coalesce(new_block, 0L))\n  ) %&gt;%\n  group_by(.data[[country_col]], block_id) %&gt;%\n  summarise(\n    streak_weeks = n(), start = min(week), end = max(week),\n    .groups = \"drop\"\n  ) %&gt;%\n  group_by(.data[[country_col]]) %&gt;%\n  slice_max(streak_weeks, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  slice_max(streak_weeks, n = 12) %&gt;%                 # show top 12 longest streaks\n  arrange(streak_weeks)\n\nggplot(sg_streaks,\n       aes(x = streak_weeks, y = reorder(.data[[country_col]], streak_weeks))) +\n  geom_col() +\n  labs(\n    title = \"Squid Game — Longest Continuous Top-10 Run by Country\",\n    x = \"Consecutive Weeks in Top-10\", y = \"Country\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# pull the single longest streak for inline copy\nsg_longest_streak &lt;- sg_streaks %&gt;% slice_tail(n = 1)"
  },
  {
    "objectID": "mp01.html#stranger-things-paragraph-helper",
    "href": "mp01.html#stranger-things-paragraph-helper",
    "title": "mp01",
    "section": "Stranger Things Paragraph Helper",
    "text": "Stranger Things Paragraph Helper\n\n\nCode\n# convenience values for the paragraph\nst_top_hours &lt;- st_by_season %&gt;% dplyr::slice_max(hours, n = 1)\nst_top_hours_share &lt;- st_top_hours$hours / st_total_hours\nst_n_seasons &lt;- st_by_season %&gt;% dplyr::filter(!is.na(season_title)) %&gt;% nrow()"
  },
  {
    "objectID": "mp01.html#stranger-things-press-release",
    "href": "mp01.html#stranger-things-press-release",
    "title": "mp01",
    "section": "Stranger Things Press Release",
    "text": "Stranger Things Press Release\nHeadline: Stranger Things surges worldwide as fans stream 2,967,980,000 hours across 3 seasons.\nWith the final chapter approaching, Stranger Things remains a global phenomenon. Viewers have streamed 2,967,980,000 hours in total, with Stranger Things 4 alone accounting for 63.6% of lifetime viewing. The franchise’s staying power is clear: Stranger Things 4 logged 19 distinct weeks in Netflix’s global Top 10, and the series has charted in 93 countries. Momentum is visible in the rapid climb of cumulative hours following recent releases, underscoring the show’s broad, durable appeal ahead of the upcoming season."
  },
  {
    "objectID": "mp01.html#india-paragraph-helper",
    "href": "mp01.html#india-paragraph-helper",
    "title": "mp01",
    "section": "India Paragraph Helper",
    "text": "India Paragraph Helper\n\n\nCode\n# ---- India PR helper (combined: data + neat sentences) ----\nsuppressPackageStartupMessages({library(dplyr); library(glue)})\n\nif (!exists(\"country_col\")) country_col &lt;- \"country_name\"\n\nindia &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(.data[[country_col]] == \"India\") %&gt;%\n  arrange(week)\n\n# #1 by week (one per week)\nindia_no1 &lt;- india %&gt;%\n  group_by(week) %&gt;%\n  slice_min(weekly_rank, with_ties = FALSE) %&gt;%\n  ungroup()\n\nno1_by_title &lt;- india_no1 %&gt;%\n  count(show_title, name = \"weeks_at_no1\") %&gt;%\n  arrange(desc(weeks_at_no1))\n\n# Longest consecutive #1 streaks\nstreaks &lt;- india_no1 %&gt;%\n  mutate(change   = show_title != lag(show_title, default = first(show_title)),\n         streak_id = cumsum(change)) %&gt;%\n  group_by(streak_id, show_title) %&gt;%\n  summarise(streak_weeks = n(),\n            start = min(week), end = max(week), .groups = \"drop\") %&gt;%\n  arrange(desc(streak_weeks))\n\n# Top persistent TV titles (weeks in Top-10)\nindia_top_titles &lt;- india %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(weeks = n_distinct(week),\n            best_rank = min(weekly_rank, na.rm = TRUE),\n            .groups = \"drop\")\n\nindia_tv_top &lt;- india_top_titles %&gt;%\n  filter(grepl(\"^TV\", category)) %&gt;%\n  slice_max(weeks, n = 10, with_ties = FALSE) %&gt;%\n  arrange(desc(weeks))\n\n# Totals for paragraph\nind_total_no1_weeks &lt;- nrow(india_no1)\n\n# --- Nicely phrased sentences ---\noxford &lt;- function(x){\n  if (length(x) &lt;= 1) return(paste0(x))\n  if (length(x) == 2) return(paste(x, collapse = \" and \"))\n  paste0(paste(x[-length(x)], collapse = \", \"), \", and \", x[length(x)])\n}\n\n# Biggest #1 winner(s)\ntop_no1_weeks  &lt;- max(no1_by_title$weeks_at_no1, na.rm = TRUE)\ntop_no1_titles &lt;- no1_by_title %&gt;% filter(weeks_at_no1 == top_no1_weeks) %&gt;% pull(show_title)\n\nind_no1_sentence &lt;- if (length(top_no1_titles) == 1) {\n  glue(\"The biggest #1 winner was **{top_no1_titles}**, holding the top spot for **{top_no1_weeks}** weeks.\")\n} else {\n  glue(\"The biggest #1 winners were **{oxford(top_no1_titles)}**, each holding the top spot for **{top_no1_weeks}** weeks.\")\n}\n\n# Longest streak(s)\nlong_weeks   &lt;- max(streaks$streak_weeks, na.rm = TRUE)\nlong_streaks &lt;- streaks %&gt;%\n  filter(streak_weeks == long_weeks) %&gt;%\n  mutate(range = glue(\"{start}–{end}\")) %&gt;%\n  arrange(show_title)\n\nif (nrow(long_streaks) == 1) {\n  ind_streak_sentence &lt;- glue(\n    \"The longest consecutive run was **{long_weeks}** weeks by **{long_streaks$show_title}** ({long_streaks$range}).\"\n  )\n} else {\n  shown &lt;- head(long_streaks, 3)\n  bits  &lt;- glue(\"**{shown$show_title}** ({shown$range})\")\n  more  &lt;- if (nrow(long_streaks) &gt; 3) \" (ties omitted)\" else \"\"\n  ind_streak_sentence &lt;- glue(\n    \"The longest consecutive run was **{long_weeks}** weeks, achieved by {oxford(bits)}{more}.\"\n  )\n}\n\n# Persistence sentence\nind_persist_sentence &lt;- glue(\n  \"Beyond weekly leaders, staying power in the Top-10 was dominated by series such as **{oxford(head(india_tv_top$show_title, 3))}**.\"\n)"
  },
  {
    "objectID": "mp01.html#press-release-2-commercial-success-in-india",
    "href": "mp01.html#press-release-2-commercial-success-in-india",
    "title": "mp01",
    "section": "Press Release 2 — Commercial Success in India",
    "text": "Press Release 2 — Commercial Success in India\nHeadline: Netflix India keeps the hits coming, with consistent #1 winners and long Top-10 runs.\nIndia’s audience continues to show strong engagement on Netflix. Across the period observed, titles captured the #1 spot in the country for 222 distinct weeks. The biggest #1 winners were Haseen Dillruba, Lucky Baskhar, and Thalaivii, each holding the top spot for 4 weeks. The longest consecutive run was 4 weeks, achieved by Haseen Dillruba (2021-07-04–2021-07-25), Lucky Baskhar (2024-12-01–2024-12-22), and Thalaivii (2021-09-26–2021-10-17). Beyond weekly leaders, staying power in the Top-10 was dominated by series such as Squid Game, Money Heist, and The Great Indian Kapil Show."
  },
  {
    "objectID": "mp01.html#squid-games-paragraph-helper",
    "href": "mp01.html#squid-games-paragraph-helper",
    "title": "mp01",
    "section": "Squid Games Paragraph Helper",
    "text": "Squid Games Paragraph Helper\n\n\nCode\n# ---- Squid Game PR helper (self-contained) ----\nsuppressPackageStartupMessages({library(dplyr); library(glue)})\n\nif (!exists(\"hours_col_g\")) hours_col_g &lt;- \"weekly_hours_viewed\"\nif (!exists(\"country_col\"))  country_col  &lt;- \"country_name\"\n\ntitle_of_interest &lt;- \"Squid Game\"\n\nsg_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(grepl(\"^TV\", category), show_title == title_of_interest) %&gt;%\n  arrange(week)\n\nsg_country &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(show_title == title_of_interest)\n\nsg_total_hours   &lt;- sg_global %&gt;%\n  summarise(total = sum(.data[[hours_col_g]], na.rm = TRUE)) %&gt;%\n  pull(total)\n\nsg_weeks_global  &lt;- dplyr::n_distinct(sg_global$week)\nsg_country_reach &lt;- sg_country %&gt;%\n  summarise(n = dplyr::n_distinct(.data[[country_col]])) %&gt;%\n  pull(n)\n\nsg_by_season &lt;- sg_global %&gt;%\n  dplyr::mutate(season_title = ifelse(is.na(season_title), \"Unknown/All\", season_title)) %&gt;%\n  dplyr::group_by(season_title) %&gt;%\n  dplyr::summarise(\n    weeks = dplyr::n_distinct(week),\n    hours = sum(.data[[hours_col_g]], na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nsg_top_season       &lt;- sg_by_season %&gt;% dplyr::slice_max(hours, n = 1)\nsg_top_season_share &lt;- sg_top_season$hours / sg_total_hours\nsg_n_seasons        &lt;- sg_by_season %&gt;% dplyr::filter(season_title != \"Unknown/All\") %&gt;% nrow()\n\nsg_top3 &lt;- sg_country %&gt;%\n  dplyr::group_by(.data[[country_col]]) %&gt;%\n  dplyr::summarise(weeks = dplyr::n_distinct(week), .groups = \"drop\") %&gt;%\n  dplyr::arrange(dplyr::desc(weeks)) %&gt;%\n  dplyr::slice_head(n = 3) %&gt;%\n  dplyr::pull(.data[[country_col]])\n\nsg_streaks &lt;- sg_country %&gt;%\n  dplyr::arrange(.data[[country_col]], week) %&gt;%\n  dplyr::group_by(.data[[country_col]]) %&gt;%\n  dplyr::mutate(\n    gap       = as.integer(week) - as.integer(dplyr::lag(week)),\n    new_block = ifelse(is.na(gap) | gap &gt; 7, 1L, 0L),\n    block_id  = cumsum(coalesce(new_block, 0L))\n  ) %&gt;%\n  dplyr::group_by(.data[[country_col]], block_id) %&gt;%\n  dplyr::summarise(\n    streak_weeks = dplyr::n(),\n    start = min(week), end = max(week),\n    .groups = \"drop\"\n  ) %&gt;%\n  dplyr::group_by(.data[[country_col]]) %&gt;%\n  dplyr::slice_max(streak_weeks, n = 1, with_ties = FALSE) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::slice_max(streak_weeks, n = 1, with_ties = TRUE)\n\nsg_longest_one &lt;- sg_streaks %&gt;% dplyr::slice_head(n = 1)\n\n\n\nPress Release 3 — Squid Game extends its global dominance\nHeadline: Squid Game reaches fans worldwide, crossing 5,048,300,000 hours over 42 weeks in Netflix’s global Top-10.\nSquid Game continues to deliver blockbuster engagement on Netflix. To date, viewers have streamed 5,048,300,000 hours across 3 seasons, with Squid Game: Season 1 contributing 54.1% of lifetime viewing. The series has charted in 94 countries and sustained long national runs—its longest continuous Top-10 streak reached 25 weeks (e.g., India, 2021-09-19–2022-03-06). Top markets by weeks in the Top-10 include India, Bangladesh, Pakistan, reflecting durable, multinational demand that spikes around major release windows."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Hi my name is Tenchi Chen, I’m a current Baruch graduate student that will be graduating in May 2026. I have a background in HR and I’m currently working on transitioning into a more data driven role. Feel free to connect with me on Linkedin if you want to learn more at https://www.linkedin.com/in/tenchi/ . Project portfolio and resume are available by request!\n\nLast Updated: Sunday 10 05, 2025 at 14:41PM"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "",
    "text": "NoteData download scripts (for transparency)\n\n\n\n\n\n02_download_tinycensus.R\nr # ===== Step A — Setup (clean session friendly) ===== if(!dir.exists(file.path(“data”, “mp02”))){ dir.create(file.path(“data”, “mp02”), showWarnings = FALSE, recursive = TRUE) }\nlibrary &lt;- function(pkg){ pkg &lt;- as.character(substitute(pkg)) options(repos = c(CRAN = “https://cloud.r-project.org”)) if(!require(pkg, character.only = TRUE, quietly = TRUE)) install.packages(pkg) stopifnot(require(pkg, character.only = TRUE, quietly = TRUE)) }\n\n\nlibrary(tidyverse) # includes dplyr, readr, purrr, stringr, etc. library(dplyr) # attach explicitly to ensure verbs are on the path library(glue) library(readxl) library(tidycensus)\n\n\n\n\n\n\n\n\n\nget_acs_all_years &lt;- function(variable, geography = “cbsa”, start_year = 2009, end_year = 2023){ fname &lt;- glue::glue(“{variable}{geography}{start_year}_{end_year}.csv”) fname &lt;- file.path(“data”, “mp02”, fname)\nif(!file.exists(fname)){ YEARS &lt;- setdiff(seq(start_year, end_year), 2020) # skip 2020 ACS1 ALL_DATA &lt;- purrr::map(YEARS, function(yy){ tidycensus::get_acs(geography, variable, year = yy, survey = “acs1”) |&gt; dplyr::mutate(year = yy) |&gt; dplyr::select(-moe, -variable) |&gt; dplyr::rename(!!variable := estimate) }) |&gt; dplyr::bind_rows() readr::write_csv(ALL_DATA, fname) }\nreadr::read_csv(fname, show_col_types = FALSE) }\n\n\n\nINCOME &lt;- get_acs_all_years(“B19013_001”) |&gt; dplyr::rename(household_income = B19013_001)\nRENT &lt;- get_acs_all_years(“B25064_001”) |&gt; dplyr::rename(monthly_rent = B25064_001)\nPOPULATION &lt;- get_acs_all_years(“B01003_001”) |&gt; dplyr::rename(population = B01003_001)\nHOUSEHOLDS &lt;- get_acs_all_years(“B11001_001”) |&gt; dplyr::rename(households = B11001_001)\n02_download_newhousingunits.R\nr # — prereqs (safe to run again) — library(dplyr); library(readr); library(readxl) library(glue); library(stringr); library(purrr) dir.create(“data/mp02”, recursive = TRUE, showWarnings = FALSE)\n\n\n\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){ out_csv &lt;- file.path(“data”, “mp02”, glue(“housing_units_{start_year}_{end_year}.csv”))\nif(!file.exists(out_csv)){\n# ---------- 1) Historical TXT (&lt;= 2018) ----------\nhist_years &lt;- seq(start_year, min(2018, end_year))\nHIST &lt;- map(hist_years, function(yy){\n  url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n  lines &lt;- readLines(url, warn = FALSE)[-(1:11)]  # drop header lines\n  cbsa_lines &lt;- str_detect(lines, \"^\\\\d{1}:\\\\d{1,}\")\n  CBSA &lt;- as.integer(str_sub(lines[cbsa_lines], 5, 10))\n  permit_lines &lt;- str_detect(str_sub(lines, 48, 53), \"^\\\\d{1,}\")\n  PERMITS &lt;- as.integer(str_sub(lines[permit_lines], 48, 53))\n  tibble(CBSA = CBSA, new_housing_units_permitted = PERMITS, year = yy)\n}) |&gt; bind_rows()\n\n# ---------- 2) Annual Excel files (2019+) ----------\ncur_years &lt;- if (end_year &gt;= 2019) seq(2019, end_year) else integer(0)\n\n# helper: try to read as xlsx first, then xls; return NULL on failure\nread_excel_safely &lt;- function(path){\n  x &lt;- try(read_xlsx(path, skip = 5), silent = TRUE)\n  if (inherits(x, \"try-error\")) {\n    x &lt;- try(read_xls(path,  skip = 5), silent = TRUE)\n  }\n  if (inherits(x, \"try-error\")) NULL else x\n}\n\n# helper: attempt several plausible URLs; only accept if we can read it as Excel\ncandidates_for &lt;- function(yy){\n  c(\n    glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\"),\n    glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xlsx\"),\n    glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}.xls\"),\n    glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}.xlsx\")\n  )\n}\n\ntry_one_year &lt;- function(yy){\n  urls &lt;- candidates_for(yy)\n  dat &lt;- NULL\n  for (u in urls){\n    tmp &lt;- tempfile(fileext = \".bin\")\n    status &lt;- try(utils::download.file(u, tmp, mode = \"wb\", quiet = TRUE), silent = TRUE)\n    # require successful download, non-tiny file, and readable Excel\n    if (!inherits(status, \"try-error\") && file.exists(tmp) && file.info(tmp)$size &gt; 5000){\n      dat &lt;- read_excel_safely(tmp)\n      if (!is.null(dat)) {  # success!\n        break\n      }\n    }\n  }\n  if (is.null(dat)) stop(\"Could not locate a valid MSA annual Excel for \", yy, \".\")\n  dat |&gt;\n    tidyr::drop_na() |&gt;\n    select(CBSA, Total) |&gt;\n    mutate(year = yy) |&gt;\n    rename(new_housing_units_permitted = Total)\n}\n\nCUR &lt;- map(cur_years, try_one_year) |&gt; bind_rows()\n\nALL &lt;- bind_rows(HIST, CUR)\nwrite_csv(ALL, out_csv)\n}\nread_csv(out_csv, show_col_types = FALSE) }\n\n\n\nPERMITS &lt;- get_building_permits() glimpse(PERMITS) dir(“data/mp02”, full.names = TRUE)\n02_download_industrycodes.R\nr # ===== Step C — BLS NAICS industry codes (robust fetch + cache) ===== library(dplyr); library(readr); library(stringr); library(purrr) library(rvest); library(httr2); library(glue); library(tibble) dir.create(“data/mp02”, recursive = TRUE, showWarnings = FALSE)\nget_bls_industry_codes &lt;- function(){ out_csv &lt;- file.path(“data”, “mp02”, “bls_industry_codes.csv”) if (!file.exists(out_csv)) { url &lt;- “https://www.bls.gov/cew/classifications/industry/industry-titles.htm”\n# Try a simple HTML fetch first (often bypasses 403)\npage &lt;- tryCatch(\n  rvest::read_html(url),\n  error = function(e) NULL\n)\n\n# Fallback: httr2 with fuller headers + retry\nif (is.null(page)) {\n  resp &lt;- httr2::request(url) |&gt;\n    httr2::req_headers(\n      `User-Agent`      = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n      `Accept`          = \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n      `Accept-Language` = \"en-US,en;q=0.9\",\n      `Referer`         = \"https://www.bls.gov/cew/classifications/industry/\"\n    ) |&gt;\n    httr2::req_retry(max_tries = 5) |&gt;\n    httr2::req_perform()\n  httr2::resp_check_status(resp)\n  page &lt;- httr2::resp_body_html(resp)\n}\n\n# Parse the table and build the leveled lookup\nraw_tbl &lt;- page |&gt;\n  rvest::html_element(\"#naics_titles\") |&gt;\n  rvest::html_table() |&gt;\n  tibble::as_tibble() |&gt;\n  rename(title = `Industry Title`) |&gt;\n  mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1L, NA_integer_)) |&gt;\n  filter(!is.na(depth))\n\nlvl4 &lt;- raw_tbl |&gt;\n  filter(depth == 4) |&gt;\n  rename(level4_title = title) |&gt;\n  mutate(\n    level1_code = str_sub(Code, end = 2),\n    level2_code = str_sub(Code, end = 3),\n    level3_code = str_sub(Code, end = 4)\n  )\n\nlvl1 &lt;- raw_tbl |&gt; select(Code, title) |&gt; rename(level1_code = Code, level1_title = title)\nlvl2 &lt;- raw_tbl |&gt; select(Code, title) |&gt; rename(level2_code = Code, level2_title = title)\nlvl3 &lt;- raw_tbl |&gt; select(Code, title) |&gt; rename(level3_code = Code, level3_title = title)\n\nnaics &lt;- lvl4 |&gt;\n  left_join(lvl1, by = \"level1_code\") |&gt;\n  left_join(lvl2, by = \"level2_code\") |&gt;\n  left_join(lvl3, by = \"level3_code\") |&gt;\n  transmute(\n    level1_code, level1_title,\n    level2_code, level2_title,\n    level3_code, level3_title,\n    level4_code = Code,\n    level4_title\n  )\n\nreadr::write_csv(naics, out_csv)\n}\nreadr::read_csv(out_csv, show_col_types = FALSE) }\nINDUSTRY_CODES &lt;- get_bls_industry_codes() glimpse(INDUSTRY_CODES) dir(“data/mp02”, full.names = TRUE) # should now include bls_industry_codes.csv\n02_download_QCEWwagesandemployment.R\nr # ===== Step D — QCEW annual averages (try-many-URLs download + cache) ===== library(dplyr); library(readr); library(stringr); library(purrr); library(glue); library(httr2)\ndir.create(“data/mp02”, recursive = TRUE, showWarnings = FALSE)\n.qcew_url_candidates &lt;- function(yy){ # Try both hosts and both filename patterns (with and without ‘qcew_’) base &lt;- c(“https://www.bls.gov”, “https://data.bls.gov”) paths &lt;- c( glue(“/cew/data/files/{yy}/csv/{yy}_qcew_annual_singlefile.zip”), glue(“/cew/data/files/{yy}/csv/{yy}_annual_singlefile.zip”) ) as.vector(outer(base, paths, paste0)) }\n.download_zip_try &lt;- function(url, dest){ req &lt;- request(url) |&gt; req_headers( User-Agent = “Mozilla/5.0 (Windows NT 10.0; Win64; x64)”, Accept = “application/zip,application/octet-stream,/”, Accept-Language = “en-US,en;q=0.9”, Referer = “https://www.bls.gov/cew/downloadable-data-files.htm”, Connection = “keep-alive” ) |&gt; req_retry(max_tries = 4, backoff = ~ runif(1, 0.5, 1.5), is_transient = function(resp) resp_status(resp) &gt;= 500)\nresp &lt;- try(req_perform(req), silent = TRUE) if (inherits(resp, “try-error”)) return(FALSE) if (resp_status(resp) != 200) return(FALSE)\nwriteBin(resp_body_raw(resp), dest) # crude size check to avoid HTML error pages saved as .zip file.exists(dest) && file.info(dest)$size &gt; 1e6 }\nget_bls_qcew_annual_averages &lt;- function(start_year = 2009, end_year = 2023){ out_csv &lt;- file.path(“data”, “mp02”, glue(“bls_qcew_{start_year}_{end_year}.csv.gz”)) YEARS &lt;- setdiff(seq(start_year, end_year), 2020)\nif (!file.exists(out_csv)) { one_year &lt;- function(yy){ zip_path &lt;- file.path(“data”, “mp02”, glue(“{yy}_qcew_annual_singlefile.zip”)) if (!file.exists(zip_path) || file.info(zip_path)$size &lt; 1e6) { message(“Downloading QCEW”, yy, ” …“) ok &lt;- FALSE tried &lt;- character(0) for (u in .qcew_url_candidates(yy)) { tried &lt;- c(tried, u) if (.download_zip_try(u, zip_path)) { ok &lt;- TRUE; break } } if (!ok) stop(”No working QCEW URL for “, yy,”. Tried:“, paste0(” - “, tried, collapse =”“)) }\n  tmpdir &lt;- tempfile(pattern = paste0(\"qcew_\", yy, \"_\")); dir.create(tmpdir)\n  files &lt;- utils::unzip(zip_path, exdir = tmpdir)\n  csvs  &lt;- files[grepl(\"\\\\.csv$\", files, ignore.case = TRUE)]\n  if (!length(csvs)) stop(\"No CSV found inside QCEW ZIP for \", yy, \".\")\n  \n  readr::read_csv(csvs[1], show_col_types = FALSE) |&gt;\n    dplyr::select(area_fips, industry_code, annual_avg_emplvl, total_annual_wages) |&gt;\n    dplyr::filter(stringr::str_starts(area_fips, \"C\"),\n                  nchar(industry_code) &lt;= 5,\n                  !stringr::str_detect(industry_code, \"-\")) |&gt;\n    dplyr::mutate(\n      YEAR        = yy,\n      FIPS        = area_fips,\n      INDUSTRY    = as.integer(industry_code),\n      EMPLOYMENT  = as.integer(annual_avg_emplvl),\n      TOTAL_WAGES = as.numeric(total_annual_wages),\n      AVG_WAGE    = dplyr::if_else(EMPLOYMENT &gt; 0, TOTAL_WAGES / EMPLOYMENT, NA_real_)\n    ) |&gt;\n    dplyr::select(FIPS, YEAR, INDUSTRY, EMPLOYMENT, TOTAL_WAGES, AVG_WAGE) |&gt;\n    dplyr::filter(INDUSTRY != 10)\n}\n\nALL &lt;- purrr::map_dfr(YEARS, one_year)\nreadr::write_csv(ALL, out_csv)\n}\ndat &lt;- readr::read_csv(out_csv, show_col_types = FALSE) missing &lt;- setdiff(YEARS, unique(dat$YEAR)) if (length(missing)) stop( “QCEW missing years:”, paste(missing, collapse = “,”), “. If a ZIP is corrupted, delete it from data/mp02 and re-run.” ) dat }\n\n\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\n\n\ndplyr::glimpse(WAGES) WAGES |&gt; count(YEAR) |&gt; arrange(YEAR) WAGES |&gt; summarise(rows = n(), n_cbsa = n_distinct(FIPS), n_ind = n_distinct(INDUSTRY))"
  },
  {
    "objectID": "mp02.html#task-7-policy-brief-elevator-pitch",
    "href": "mp02.html#task-7-policy-brief-elevator-pitch",
    "title": "Mini-Project #02: Making Backyards Affordable for All",
    "section": "Task 7 — Policy Brief (Elevator Pitch)",
    "text": "Task 7 — Policy Brief (Elevator Pitch)\nPolicy Brief: Federal YIMBY Partnership Act\nOur analysis shows metros that permit more homes per resident tend to see rent-burden fall even as population grows. The Federal YIMBY Partnership Act rewards cities that adopt proven pro-housing reforms—by-right infill/ADUs, parking reform, and fast digital permitting—and then deliver measurable results. It’s a performance grant, not a mandate: more homes, lower rent-burden, transparent metrics.\n\nWhat it does: Competitive grants for cities that modernize zoning & permitting and hit outcomes on supply and affordability.\nWhy now: Employers can’t hire if workers can’t live near jobs; lower rent-burden means more take-home pay circulating locally.\nWho backs it: Health-care workers (staffing & retention) and building trades (steady, predictable projects).\nHow we’ll measure success:\n\nRent-Burden Index ↓ (latest vs early period)\nPermits / housing growth (instant, z) ↑\n5-year housing growth vs population ↑\n\n\nProposed sponsors (with local facts):\n\nPrimary — Houston-Sugar Land-Baytown, TX Metro Area: Rent-Burden Index 91.1 → 104.9 (Δ 13.8 points; lower is better). Housing growth (instant, z): 189.3. Population growth: 5.8%.\nCo-sponsor — New York-Northern New Jersey-Long Island, NY-NJ-PA Metro Area: Rent-Burden Index 104.1 → 107.8 (Δ 3.7 points). Housing growth (instant, z): 66.8. Population growth: 0.5%.\n\nWhat we’re asking: Sponsor a bill that rewards results—cities unlock multi-year funds when they build more homes and cut rent-burden."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "title: “Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC” author: “Tenchi Chen” format: html: theme: cosmo toc: true toc-depth: 2 code-fold: true # makes every chunk collapsible code-summary: “Show code” df-print: paged execute: echo: true warning: false message: false —"
  },
  {
    "objectID": "mp03.html#data-acquisition",
    "href": "mp03.html#data-acquisition",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Data Acquisition",
    "text": "Data Acquisition\n\nNYC City Council Districts"
  },
  {
    "objectID": "mp03.html#task-1-download-nyc-city-council-district-boundaries",
    "href": "mp03.html#task-1-download-nyc-city-council-district-boundaries",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 1: Download NYC City Council District Boundaries",
    "text": "Task 1: Download NYC City Council District Boundaries\n\n\nShow code\n# load required library\nlibrary(sf)\n\n# Function to download NYC City Council District Boundaries responsibly\ndownload_nyc_districts &lt;- function() {\n  \n  # i. Create directory data/mp03 within the data folder (only if needed)\n  if (!dir.exists(\"data/mp03\")) {\n    dir.create(\"data/mp03\", recursive = TRUE)\n  }\n  \n  # ii. Download the zip file (only if needed)\n  zip_path &lt;- \"data/mp03/nycc_25c.zip\"\n  if (!file.exists(zip_path)) {\n    download.file(\n      url = \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\",\n      destfile = zip_path,\n      mode = \"wb\"\n    )\n  }\n  \n  # iii. Unzip only if no shapefile exists yet\n  shp_files &lt;- list.files(\n    \"data/mp03\",\n    pattern = \"\\\\.shp$\",\n    recursive = TRUE,\n    full.names = TRUE\n  )\n  \n  if (length(shp_files) == 0) {\n    unzip(zip_path, exdir = \"data/mp03\")\n    shp_files &lt;- list.files(\n      \"data/mp03\",\n      pattern = \"\\\\.shp$\",\n      recursive = TRUE,\n      full.names = TRUE\n    )\n  }\n  \n  # safety check: make sure we actually have a .shp file\n  if (length(shp_files) == 0) {\n    stop(\"No .shp file found after unzipping.\")\n  }\n  \n  # use the first shapefile found (e.g., data/mp03/nycc_25c/nycc.shp)\n  shp_file &lt;- shp_files[1]\n  \n  message(\"Reading shapefile from: \", shp_file)\n  districts &lt;- st_read(shp_file, quiet = TRUE)\n  \n  # v. Transform to WGS 84 coordinate system\n  districts_wgs84 &lt;- st_transform(districts, crs = \"WGS84\")\n  \n  # vi. Return the transformed data\n  districts_wgs84\n}\n\n# Call the function and store the result\nnyc_districts &lt;- download_nyc_districts()\n\n# Display basic information about the data\nnyc_districts\n\n\n\n  \n\n\n\n\nNYC Tree Points"
  },
  {
    "objectID": "mp03.html#task-2-download-tree-points",
    "href": "mp03.html#task-2-download-tree-points",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 2: Download Tree Points",
    "text": "Task 2: Download Tree Points\n\n\nShow code\nlibrary(httr2)\nlibrary(dplyr)\nlibrary(sf)\n\ndownload_tree_points &lt;- function(limit = 1000) {\n  \n  # base API endpoint (GeoJSON)\n  base_url &lt;- \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\"\n  \n  # ensure data/mp03 directory exists\n  dir_path &lt;- \"data/mp03\"\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  # list to store sf pages\n  sf_pages &lt;- list()\n  \n  offset &lt;- 0L\n  page   &lt;- 1L\n  \n  repeat {\n    # file name for this page\n    file_path &lt;- file.path(\n      dir_path,\n      sprintf(\"tree_points_%05d.geojson\", offset)\n    )\n    \n    # only hit the API if this page is not already saved\n    if (!file.exists(file_path)) {\n      url &lt;- paste0(\n        base_url,\n        \"?$limit=\",  limit,\n        \"&$offset=\", offset\n      )\n      \n      # perform request but do not throw on HTTP errors\n      req &lt;- httr2::request(url) |&gt;\n        httr2::req_error(is_error = function(resp) FALSE)\n      \n      resp   &lt;- httr2::req_perform(req)\n      status &lt;- httr2::resp_status(resp)\n      \n      # if the server says bad request / no more data, stop paging\n      if (status &gt;= 400) {\n        message(\"Stopping download at offset \", offset,\n                \" (HTTP status \", status, \").\")\n        break\n      }\n      \n      # save response body as GeoJSON file\n      writeBin(httr2::resp_body_raw(resp), file_path)\n    }\n    \n    # read this page\n    sf_page &lt;- sf::st_read(file_path, quiet = TRUE)\n    \n    # if the page is empty, no more data\n    if (nrow(sf_page) == 0) {\n      break\n    }\n    \n    # make sure planteddate has the SAME type on every page\n    if (\"planteddate\" %in% names(sf_page)) {\n      sf_page$planteddate &lt;- as.character(sf_page$planteddate)\n    }\n    \n    # store page\n    sf_pages[[page]] &lt;- sf_page\n    \n    # if we got fewer than `limit` rows, this was the last page\n    if (nrow(sf_page) &lt; limit) {\n      break\n    }\n    \n    # otherwise move on to next page\n    offset &lt;- offset + limit\n    page   &lt;- page + 1L\n  }\n  \n  # combine all pages into a single sf object\n  dplyr::bind_rows(sf_pages)\n}\n\nnyc_trees &lt;- download_tree_points(limit = 1000)\nnyc_trees"
  },
  {
    "objectID": "mp03.html#data-integration-and-initial-exploration",
    "href": "mp03.html#data-integration-and-initial-exploration",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Data Integration and Initial Exploration",
    "text": "Data Integration and Initial Exploration\n\nMapping NYC Trees"
  },
  {
    "objectID": "mp03.html#task-3-plot-all-tree-points",
    "href": "mp03.html#task-3-plot-all-tree-points",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 3: Plot All Tree Points",
    "text": "Task 3: Plot All Tree Points\n\n\nShow code\n# load ggplot2 for mapping\nlibrary(ggplot2)\n\n# ggplot map with council districts (polygons) and tree points\nggplot() +\n  # council district boundaries layer\n  geom_sf(\n    data = nyc_districts,\n    fill = NA,\n    color = \"grey60\",\n    linewidth = 0.3\n  ) +\n  # tree points layer\n  geom_sf(\n    data = nyc_trees,\n    color = \"darkgreen\",\n    alpha = 0.2,\n    size = 0.1\n  ) +\n  coord_sf() +\n  theme_minimal() +\n  labs(\n    title = \"NYC Tree Points over City Council Districts\",\n    subtitle = \"Each point is a tree; polygons show City Council districts\",\n    x = NULL,\n    y = NULL\n  )"
  },
  {
    "objectID": "mp03.html#district-level-analyses-of-trees",
    "href": "mp03.html#district-level-analyses-of-trees",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "District-Level Analyses of Trees",
    "text": "District-Level Analyses of Trees\n\nTask 4: District-Level Analysis of Tree Coverage"
  },
  {
    "objectID": "mp03.html#join-code",
    "href": "mp03.html#join-code",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Join Code",
    "text": "Join Code\n\n\nShow code\nlibrary(dplyr)\nlibrary(sf)\n\n# spatial join: give each tree its council district info\ntrees_with_district &lt;- st_join(\n  nyc_trees,\n  nyc_districts,\n  join = st_intersects\n)\n\n# add borough based on council district number\ntrees_with_district &lt;- trees_with_district |&gt;\n  mutate(\n    borough = case_when(\n      CounDist &gt;= 1  & CounDist &lt;= 10 ~ \"Manhattan\",\n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\",\n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\",\n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# drop geometry for attribute summaries\ntree_attrs &lt;- trees_with_district |&gt;\n  st_drop_geometry()\n\n# automatically find a species-like column name\nspecies_candidates &lt;- grep(\"spc|species\", names(tree_attrs),\n                           ignore.case = TRUE, value = TRUE)\n\n#species_candidates   # print for check\n\nspecies_col &lt;- if (length(species_candidates) &gt; 0) {\n  species_candidates[1]   # use the first match by default\n} else {\n  NA_character_\n}\n\n#species_col            # print for check"
  },
  {
    "objectID": "mp03.html#which-council-district-has-the-most-trees",
    "href": "mp03.html#which-council-district-has-the-most-trees",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "1. Which council district has the most trees?",
    "text": "1. Which council district has the most trees?\n\n\nShow code\n# drop geometry for faster summaries and create a nice district name\ntree_attrs &lt;- trees_with_district |&gt;\n  st_drop_geometry() |&gt;\n  mutate(\n    district_name = paste(borough, \"District\", CounDist)\n  )\n\n# 1. Which council district has the most trees?\n\nq1_most_trees &lt;- tree_attrs |&gt;\ngroup_by(CounDist, borough, district_name) |&gt;\nsummarise(n_trees = n(), .groups = \"drop\") |&gt;\narrange(desc(n_trees))\n\nq1_most_trees_top &lt;- q1_most_trees |&gt;\nslice_max(n_trees, n = 1, with_ties = FALSE)\n\nq1_most_trees_top"
  },
  {
    "objectID": "mp03.html#which-council-district-has-the-highest-density-of-trees",
    "href": "mp03.html#which-council-district-has-the-highest-density-of-trees",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "2. Which council district has the highest density of trees?",
    "text": "2. Which council district has the highest density of trees?\n\n\nShow code\n# 2. Which council district has the highest density of trees?\n\nq2_density &lt;- tree_attrs |&gt;\ngroup_by(CounDist, borough, district_name) |&gt;\nsummarise(\nn_trees    = n(),\nShape_Area = first(Shape_Area),\n.groups    = \"drop\"\n) |&gt;\nmutate(tree_density = n_trees / Shape_Area) |&gt;\narrange(desc(tree_density))\n\nq2_density_top &lt;- q2_density |&gt;\nslice_max(tree_density, n = 1, with_ties = FALSE)\n\nq2_density_top"
  },
  {
    "objectID": "mp03.html#which-district-has-highest-fraction-of-dead-trees",
    "href": "mp03.html#which-district-has-highest-fraction-of-dead-trees",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "3. Which district has highest fraction of dead trees?",
    "text": "3. Which district has highest fraction of dead trees?\n\n\nShow code\n# 3. Which district has highest fraction of dead trees?\n\nq3_dead_fraction &lt;- tree_attrs |&gt;\ngroup_by(CounDist, borough, district_name) |&gt;\nsummarise(\nn_trees = n(),\nn_dead  = sum(tpcondition == \"Dead\", na.rm = TRUE),\n.groups = \"drop\"\n) |&gt;\nmutate(frac_dead = n_dead / n_trees) |&gt;\narrange(desc(frac_dead))\n\nq3_dead_fraction_top &lt;- q3_dead_fraction |&gt;\nslice_max(frac_dead, n = 1, with_ties = FALSE)\n\nq3_dead_fraction_top"
  },
  {
    "objectID": "mp03.html#most-common-tree-species-in-manhattan",
    "href": "mp03.html#most-common-tree-species-in-manhattan",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "4. Most common tree species in Manhattan",
    "text": "4. Most common tree species in Manhattan\n\n\nShow code\nlibrary(dplyr)\nlibrary(tibble)\n\n# safety check: make sure we actually found a species column\nif (is.na(species_col)) {\n  stop(\"No species-like column found. Check names(tree_attrs) and set `species_col` manually.\")\n}\n\n# 4. Most common tree species in Manhattan\n\nmanhattan_trees &lt;- tree_attrs |&gt;\n  filter(borough == \"Manhattan\")\n\n# pull the species column as a plain vector\nspecies_vec &lt;- manhattan_trees[[species_col]]\n\nq4_manhattan_species &lt;- tibble(\n  species = species_vec\n) |&gt;\n  filter(!is.na(species)) |&gt;\n  count(species, sort = TRUE, name = \"n_trees\")\n\nq4_manhattan_top &lt;- q4_manhattan_species |&gt;\n  slice_max(n_trees, n = 1, with_ties = FALSE)\n\nq4_manhattan_top"
  },
  {
    "objectID": "mp03.html#species-of-the-tree-closest-to-baruchs-campus",
    "href": "mp03.html#species-of-the-tree-closest-to-baruchs-campus",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "5. Species of the tree closest to Baruch’s campus",
    "text": "5. Species of the tree closest to Baruch’s campus\n\n\nShow code\n# 5. Species of the tree closest to Baruch's campus\n\n# helper to create a point with WGS84 CRS\n\nnew_st_point &lt;- function(lat, lon) {\nst_sfc(st_point(c(lon, lat))) |&gt;\nst_set_crs(\"WGS84\")\n}\n\n# approximate location of Baruch College (55 Lexington Ave)\n\nbaruch_point &lt;- new_st_point(\nlat = 40.7403,\nlon = -73.9833\n)\n\n# compute distance from each tree to Baruch\n\ntrees_with_distance &lt;- trees_with_district |&gt;\nmutate(distance_m = as.numeric(st_distance(geometry, baruch_point)))\n\nclosest_tree &lt;- trees_with_distance |&gt;\narrange(distance_m) |&gt;\nslice(1)\n\n# show species and distance of the closest tree\n\nclosest_tree |&gt;\nst_drop_geometry() |&gt;\nselect(CounDist, borough, all_of(species_col), distance_m)"
  },
  {
    "objectID": "mp03.html#extra-credit-interactive-map-additional-parks-data",
    "href": "mp03.html#extra-credit-interactive-map-additional-parks-data",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Extra Credit: Interactive Map & Additional Parks Data",
    "text": "Extra Credit: Interactive Map & Additional Parks Data\n\n\nShow code\n# Extra data: Forestry work orders and risk assessments (downloaded politely)\n\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Generic helper for Socrata JSON with $limit / $offset + local caching\ndownload_socrata_json &lt;- function(base_url, prefix, limit = 1000) {\n  \n  dir_path &lt;- \"data/mp03\"\n  if (!dir.exists(dir_path)) {\n    dir.create(dir_path, recursive = TRUE)\n  }\n  \n  offset &lt;- 0L\n  page   &lt;- 1L\n  pages  &lt;- list()\n  \n  repeat {\n    file_path &lt;- file.path(\n      dir_path,\n      sprintf(\"%s_%05d.json\", prefix, offset)\n    )\n    \n    # Only hit the API if this page is not already saved\n    if (!file.exists(file_path)) {\n      url &lt;- paste0(\n        base_url,\n        \"?$limit=\",  limit,\n        \"&$offset=\", offset\n      )\n      \n      req &lt;- httr2::request(url) |&gt;\n        httr2::req_error(is_error = function(resp) FALSE)\n      \n      resp   &lt;- httr2::req_perform(req)\n      status &lt;- httr2::resp_status(resp)\n      \n      # Stop politely if the server says \"no more\"\n      if (status &gt;= 400) {\n        message(\"Stopping download for \", prefix, \" at offset \", offset,\n                \" (HTTP status \", status, \").\")\n        break\n      }\n      \n      writeBin(httr2::resp_body_raw(resp), file_path)\n    }\n    \n    # Read JSON from the cached file\n    page_data &lt;- jsonlite::read_json(file_path, simplifyVector = TRUE)\n    \n    # If page is empty, no more data\n    if (length(page_data) == 0) {\n      break\n    }\n    \n    pages[[page]] &lt;- tibble::as_tibble(page_data)\n    \n    # If fewer than limit rows, this was the last page\n    if (nrow(pages[[page]]) &lt; limit) {\n      break\n    }\n    \n    offset &lt;- offset + limit\n    page   &lt;- page + 1L\n  }\n  \n  dplyr::bind_rows(pages)\n}\n\n# NOTE: we use the dataset IDs from your links, but the JSON API form (.json)\n# Work orders dataset (bdjm-n7q4)\nwork_orders &lt;- download_socrata_json(\n  base_url = \"https://data.cityofnewyork.us/resource/bdjm-n7q4.json\",\n  prefix   = \"work_orders\"\n)\n\n# Work assessments dataset (259a-b6s7)\nwork_assess &lt;- download_socrata_json(\n  base_url = \"https://data.cityofnewyork.us/resource/259a-b6s7.json\",\n  prefix   = \"work_assess\"\n)\n\n# Helper to auto-detect a council-district-like column name\nfind_cd_col &lt;- function(df) {\n  cd_candidates &lt;- grep(\"council|coun_dist|council_district|^cd$\",\n                        names(df), ignore.case = TRUE, value = TRUE)\n  if (length(cd_candidates) == 0) {\n    NA_character_\n  } else {\n    cd_candidates[1]\n  }\n}\n\nwo_cd_col &lt;- find_cd_col(work_orders)\nas_cd_col &lt;- find_cd_col(work_assess)\n\n# Restrict each dataset to Queens Council District 32 (if possible)\nif (!is.na(wo_cd_col)) {\n  work_orders_32 &lt;- work_orders[work_orders[[wo_cd_col]] == 32, , drop = FALSE]\n} else {\n  work_orders_32 &lt;- work_orders[0, , drop = FALSE]\n}\n\nif (!is.na(as_cd_col)) {\n  work_assess_32 &lt;- work_assess[work_assess[[as_cd_col]] == 32, , drop = FALSE]\n} else {\n  work_assess_32 &lt;- work_assess[0, , drop = FALSE]\n}\n\n# Simple counts you can use in your proposal/map\nn_work_orders_32 &lt;- nrow(work_orders_32)\nn_assess_32      &lt;- nrow(work_assess_32)"
  },
  {
    "objectID": "mp03.html#interactive-map-for-queens-council-distrtict-32-with-extra-credit-data",
    "href": "mp03.html#interactive-map-for-queens-council-distrtict-32-with-extra-credit-data",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Interactive Map For Queens Council Distrtict 32 with Extra Credit Data",
    "text": "Interactive Map For Queens Council Distrtict 32 with Extra Credit Data\n\n\nShow code\n# Interactive map for Queens Council District 32 with extra data baked in\n\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\n\n# District 32 boundary (nyc_districts and q3_dead_fraction come from earlier tasks)\ndistrict_32 &lt;- nyc_districts |&gt;\n  filter(CounDist == 32)\n\n# Trees in District 32\ntrees_32 &lt;- trees_with_district |&gt;\n  filter(CounDist == 32)\n\n# Dead-tree summary for District 32 (from your earlier Q3 calc)\ntree_summary_32 &lt;- q3_dead_fraction |&gt;\n  filter(CounDist == 32)\n\nn_trees_32  &lt;- tree_summary_32$n_trees\nn_dead_32   &lt;- tree_summary_32$n_dead\nfrac_dead32 &lt;- tree_summary_32$frac_dead\n\n# Popup text combining tree stats + work orders + assessments\npopup_text &lt;- sprintf(\n  \"Queens Council District 32&lt;br/&gt;Trees: %s&lt;br/&gt;Dead trees: %s (%.1f%%)&lt;br/&gt;Work orders: %s&lt;br/&gt;Risk assessments: %s\",\n  format(n_trees_32, big.mark = \",\"),\n  format(n_dead_32,  big.mark = \",\"),\n  100 * frac_dead32,\n  format(n_work_orders_32, big.mark = \",\"),\n  format(n_assess_32,      big.mark = \",\")\n)\n\nleaflet() |&gt;\n  addProviderTiles(\"CartoDB.Positron\") |&gt;\n  addPolygons(\n    data   = district_32,\n    weight = 2,\n    color  = \"#444444\",\n    fill   = FALSE,\n    popup  = popup_text,\n    label  = \"Queens District 32\"\n  ) |&gt;\n  addCircleMarkers(\n    data = st_transform(trees_32, 4326),\n    radius = 2,\n    stroke = FALSE,\n    fillOpacity = 0.4,\n    color = ifelse(trees_32$tpcondition == \"Dead\", \"red\", \"darkgreen\")\n  ) |&gt;\n  setView(lng = -73.85, lat = 40.67, zoom = 12)"
  },
  {
    "objectID": "mp03.html#interactive-map-for-all-district-data",
    "href": "mp03.html#interactive-map-for-all-district-data",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Interactive Map for All District Data",
    "text": "Interactive Map for All District Data\n\n\nShow code\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\n\n# Build a summary sf for each district (join district polygons with your Q3 table)\ndistrict_summary &lt;- nyc_districts |&gt;\n  left_join(\n    q3_dead_fraction |&gt; select(CounDist, n_trees, n_dead, frac_dead),\n    by = \"CounDist\"\n  ) |&gt;\n  mutate(\n    borough = case_when(\n      CounDist &gt;= 1  & CounDist &lt;= 10 ~ \"Manhattan\",\n      CounDist &gt;= 11 & CounDist &lt;= 18 ~ \"Bronx\",\n      CounDist &gt;= 19 & CounDist &lt;= 32 ~ \"Queens\",\n      CounDist &gt;= 33 & CounDist &lt;= 48 ~ \"Brooklyn\",\n      CounDist &gt;= 49 & CounDist &lt;= 51 ~ \"Staten Island\",\n      TRUE ~ NA_character_\n    )\n  )\n\n# Color palette for dead-tree fraction\npal_dead &lt;- colorNumeric(\n  palette = \"YlOrRd\",\n  domain  = district_summary$frac_dead,\n  na.color = \"#CCCCCC\"\n)\n\nleaflet() |&gt;\n  addProviderTiles(\"CartoDB.Positron\") |&gt;\n  # all districts, filled by dead-tree fraction\n  addPolygons(\n    data        = district_summary,\n    fillColor   = ~pal_dead(frac_dead),\n    fillOpacity = 0.7,\n    weight      = 1,\n    color       = \"#555555\",\n    popup       = ~sprintf(\n      \"District %d (%s)&lt;br/&gt;Trees: %s&lt;br/&gt;Dead trees: %s (%.1f%%)\",\n      CounDist,\n      borough,\n      format(n_trees, big.mark = \",\"),\n      format(n_dead,  big.mark = \",\"),\n      100 * frac_dead\n    ),\n    label       = ~paste0(\"District \", CounDist, \" (\", borough, \")\")\n  ) |&gt;\n  # outline District 32 so your proposal district pops out\n  addPolygons(\n    data  = district_summary |&gt; filter(CounDist == 32),\n    fill  = FALSE,\n    weight = 3,\n    color  = \"black\"\n  ) |&gt;\n  addLegend(\n    position = \"bottomright\",\n    pal      = pal_dead,\n    values   = district_summary$frac_dead,\n    title    = \"Fraction of dead trees\",\n    labFormat = leaflet::labelFormat(transform = function(x) 100 * x, suffix = \"%\")\n  ) |&gt;\n  setView(lng = -73.94, lat = 40.72, zoom = 10)\n\n\n\n\n\n\n\nNYC Parks Proposal: Fixing the Canopy in Queens Council District 32\nUsing the NYC tree data, we see that Queens Council District 32 stands out for the wrong reason:\n\nIt has about 1,936 trees.\nAbout 28% of them are marked as dead — the highest fraction of dead trees of any council district in our data.\nOther districts, such as Staten Island District 50 and Manhattan District 10, have more trees overall but a much lower share of dead trees.\n\nBecause of this, District 32 is a strong candidate for a focused tree program.\n\n\nProposed Project\nProject name: Reviving the Canopy in District 32\nMain goals (over 3 years):\n\nRemove and replace 500 dead or critically rated trees.\nPlant 800 new trees on blocks with few or no trees.\nReduce the dead-tree share to below 10% within five years.\n\n\n\n\nKey Visuals\n\nZoomed-in district map\n\nShow only District 32.\nPlot all trees as points:\n\nLive trees in green.\nDead trees in red.\n\nThis makes it easy to see “hot spots” with many dead trees along major streets.\n\nComparison chart with other districts\nA bar chart comparing:\n\nTotal number of trees, and\n\nFraction of dead trees\n\nfor:\n\nQueens District 32 (our focus),\nStaten Island District 50 (many trees, low mortality),\nManhattan District 10 (high tree density, healthier canopy),\nManhattan District 2 (Baruch’s district).\n\nThe idea is to show that District 32 has too many dead trees compared with other districts, even those that are dense and busy."
  },
  {
    "objectID": "mp03.html#bar-chart",
    "href": "mp03.html#bar-chart",
    "title": "Mini-Project #03: Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Bar Chart",
    "text": "Bar Chart\n\n\nShow code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Pick the districts mentioned in your write-up\ncomparison &lt;- q3_dead_fraction |&gt;\n  filter(CounDist %in% c(32, 50, 10, 2)) |&gt;\n  mutate(\n    district_label = case_when(\n      CounDist == 32 ~ \"Queens 32\",\n      CounDist == 50 ~ \"Staten Island 50\",\n      CounDist == 10 ~ \"Manhattan 10\",\n      CounDist == 2  ~ \"Manhattan 2\",\n      TRUE ~ as.character(CounDist)\n    )\n  ) |&gt;\n  select(district_label, n_trees, frac_dead)\n\n# Put total trees and % dead into long format for faceting\ncomparison_long &lt;- comparison |&gt;\n  mutate(dead_pct = 100 * frac_dead) |&gt;\n  select(district_label, n_trees, dead_pct) |&gt;\n  pivot_longer(\n    cols      = c(n_trees, dead_pct),\n    names_to  = \"metric\",\n    values_to = \"value\"\n  )\n\n# Nice labels for facets\nmetric_labels &lt;- c(\n  n_trees  = \"Total number of trees\",\n  dead_pct = \"Dead trees (%)\"\n)\n\nggplot(comparison_long, aes(x = district_label, y = value)) +\n  geom_col() +\n  facet_wrap(~metric, scales = \"free_y\",\n             labeller = as_labeller(metric_labels)) +\n  labs(\n    x = NULL,\n    y = NULL,\n    title = \"Tree Counts and Dead-Tree Percentage for Selected Districts\"\n  ) +\n  theme_minimal() +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 20, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\nActions\nThe program would fund:\n\nTree removals and stump grinding in blocks with multiple dead trees.\nReplanting using hardy, pollution-tolerant species (based on the genusspecies data and Parks’ recommended list).\nRisk inspections for large trees with poor condition or high risk ratings, to prevent future failures.\nA community “Giving Tree Day” based on the popular children’s book by Shel Silverstein which can invoke feeling of sentimentality and may drive residents to get watering bags and learn to adopt basic mannerisms on how to care for new street trees in their neighborhood.\n\n\n\n\nWhy District 32?\nDistrict 32:\n\nDoes not have the most trees overall,\n\nBut has the highest percentage of dead trees,\n\nAnd clear clusters of dead trees in the map.\n\nDirecting extra funding here would quickly improve street appearance, shade, and safety, and bring District 32 closer to the healthier tree conditions seen in Districts 50, 10, and 2."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "STA 9750 – Mini-Project 04",
    "section": "",
    "text": "Show code\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(infer)"
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Introduction",
    "text": "Introduction\nIn this mini–project, I use httr2 and rvest to replicate browser requests to the BLS website and acquire two datasets:\n\nSeasonally adjusted total nonfarm payroll levels (CES0000000001).\nRevisions to the CES nonfarm payroll employment estimates.\n\nThe code chunks below show the full workflow from HTTP requests to cleaned monthly data frames."
  },
  {
    "objectID": "mp04.html#task-1-ces-total-nonfarm-payroll-levels",
    "href": "mp04.html#task-1-ces-total-nonfarm-payroll-levels",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Task 1: CES Total Nonfarm Payroll Levels",
    "text": "Task 1: CES Total Nonfarm Payroll Levels\nIn this section, I reproduce the browser’s POST request to SurveyOutputServlet using httr2, download the HTML table of seasonally adjusted total nonfarm payroll levels from 1979–2025, and reshape it into a monthly date / level data frame.\n\nTask 1.1: Replicate the HTTP request\n\n\nShow code\nlibrary(httr2)\nlibrary(rvest)\n\nces_req &lt;- request(\"https://data.bls.gov/pdq/SurveyOutputServlet\") |&gt;\n  req_method(\"POST\") |&gt;\n  req_headers(\n    \"User-Agent\" = \"chentenchi@gmail.com STA9750 mini-project (R)\",\n    \"Referer\"    = \"https://data.bls.gov/pdq/SurveyOutputServlet\"\n  ) |&gt;\n  req_body_form(\n    request_action    = \"get_data\",\n    reformat          = \"true\",\n    from_results_page = \"true\",\n    from_year         = \"1979\",\n    to_year           = \"2025\",\n    Go.x              = \"9\",\n    Go.y              = \"4\",\n    initial_request   = \"false\",\n    data_tool         = \"surveymost\",\n    series_id         = \"CES0000000001\",\n    years_option      = \"specific_years\"\n  )\n\nces_resp &lt;- ces_req |&gt;\n  req_perform()\n\nces_resp  # check for Status: 200\n\n\n&lt;httr2_response&gt;\nPOST https://data.bls.gov/pdq/SurveyOutputServlet\nStatus: 200 OK\nContent-Type: text/html\nBody: In memory (68205 bytes)\n\n\nShow code\nces_html &lt;- ces_resp |&gt;\n  resp_body_html()\n\nces_table &lt;- ces_html |&gt;\n  html_element(\"table\") |&gt;\n  html_table(fill = TRUE)\n\nhead(ces_table)\n\n\n# A tibble: 6 × 2\n  X1                  X2                                                        \n  &lt;chr&gt;               &lt;chr&gt;                                                     \n1 Series Id:          CES0000000001                                             \n2 Seasonally Adjusted Seasonally Adjusted                                       \n3 Series Title:       All employees, thousands, total nonfarm, seasonally adjus…\n4 Super Sector:       Total nonfarm                                             \n5 Industry:           Total nonfarm                                             \n6 NAICS Code:         -                                                         \n\n\n\n\nTask 1.2: Extract and clean the CES table\n\n\nShow code\nces_table &lt;- ces_html |&gt;\nhtml_element(\"table.regular-data\") |&gt; # or just html_element(\"table\") if there’s only one\nhtml_table(fill = TRUE)\n\nhead(ces_table)\n\n\n# A tibble: 6 × 13\n  Year  Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec  \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 1979  88808 89055 89479 89417 89789 90108 90217 90300 90327 90481 90573 90672\n2 1980  90800 90883 90994 90849 90420 90101 89840 90099 90213 90490 90747 90943\n3 1981  91033 91105 91210 91283 91296 91490 91601 91565 91477 91380 91171 90895\n4 1982  90565 90563 90434 90150 90107 89865 89521 89363 89183 88907 88786 88771\n5 1983  88990 88917 89090 89364 89644 90021 90437 90129 91247 91520 91875 92230\n6 1984  92673 93157 93429 93792 94098 94479 94789 95032 95344 95629 95982 96107\n\n\n\n\nShow code\n# reshape CES table to monthly date/level\nces_monthly &lt;- ces_table |&gt;\n  filter(!is.na(Year), Year != \"\") |&gt;\n  pivot_longer(\n    cols      = Jan:Dec,\n    names_to  = \"month\",\n    values_to = \"level\"\n  ) |&gt;\n  mutate(\n    level = readr::parse_number(level),\n    date  = lubridate::my(paste(month, Year))\n  ) |&gt;\n  filter(\n    !is.na(level),\n    !is.na(date),\n    date &lt;= lubridate::ymd(\"2025-06-01\")\n  ) |&gt;\n  arrange(date) |&gt;\n  select(date, level)\n\nhead(ces_monthly)\n\n\n# A tibble: 6 × 2\n  date       level\n  &lt;date&gt;     &lt;dbl&gt;\n1 1979-01-01 88808\n2 1979-02-01 89055\n3 1979-03-01 89479\n4 1979-04-01 89417\n5 1979-05-01 89789\n6 1979-06-01 90108"
  },
  {
    "objectID": "mp04.html#task-2-ces-revisions-tables",
    "href": "mp04.html#task-2-ces-revisions-tables",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Task 2: CES Revisions Tables",
    "text": "Task 2: CES Revisions Tables\nHere I access the static CES revisions page using httr2, extract the year-specific revision tables with rvest, and build a function that returns the monthly revisions for any year. I then apply this function from 1979–2025 and construct a data frame with:\n\ndate (first day of the month),\noriginal (first estimate),\nfinal (third estimate),\nrevision = final - original.\n\n\nTask 2.1: Download revisions page\n\n\nShow code\nlibrary(httr2)\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(readr)\n\n# ---- Task 2: download CES revisions page ----\nrev_req &lt;- request(\"https://www.bls.gov/web/empsit/cesnaicsrev.htm\") |&gt;\n  req_method(\"GET\") |&gt;\n  req_headers(\n    \"User-Agent\" = \"chentenchig@gmail.com STA9750 mini-project (R)\",\n    \"Referer\"    = \"https://www.bls.gov/web/empsit/cesnaicsrev.htm\"\n  )\n\nrev_resp &lt;- rev_req |&gt;\n  req_perform()\n\nrev_resp   # check Status: 200\n\n\n&lt;httr2_response&gt;\nGET https://www.bls.gov/web/empsit/cesnaicsrev.htm\nStatus: 200 OK\nContent-Type: text/html\nBody: In memory (401469 bytes)\n\n\nShow code\nrev_html &lt;- rev_resp |&gt;\n  resp_body_html()\n\n\n\n\nShow code\n# helper: CSS selector for a year table\ntable_selector_for_year &lt;- function(year) {\n  paste0(\"table#\", year)\n}\n\n# 2024 \nrev_2024_raw &lt;- rev_html |&gt;\n  html_element(table_selector_for_year(2024)) |&gt;\n  html_table(header = FALSE, fill = TRUE)\n\nhead(rev_2024_raw, 15)\n\n\n# A tibble: 15 × 14\n   X1    X2    X3    X4    X5    X6    X7    X8    X9    X10   X11   X12   X13  \n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 Month Year  Seas… Seas… Seas… Seas… Seas… Seas… Not … Not … Not … Not … Not …\n 2 Month Year  Over… Over… Over… Revi… Revi… Revi… Over… Over… Over… Revi… Revi…\n 3 Month Year  1st   2nd   3rd   2nd … 3rd … 3rd … 1st   2nd   3rd   2nd … 3rd …\n 4 Jan.  2024  353   229   256   -124  27    -97   -2635 -2855 -2837 -220  18   \n 5 Feb.  2024  275   270   236   -5    -34   -39   1141  1127  1119  -14   -8   \n 6 Mar.  2024  303   315   310   12    -5    7     659   662   659   3     -3   \n 7 Apr.  2024  175   165   108   -10   -57   -67   803   791   791   -12   0    \n 8 May   2024  272   218   216   -54   -2    -56   917   844   841   -73   -3   \n 9 Jun.  2024  206   179   118   -27   -61   -88   547   518   499   -29   -19  \n10 Jul.  2024  114   89    144   -25   55    30    -915  -954  -942  -39   12   \n11 Aug.  2024  142   159   78    17    -81   -64   263   318   332   55    14   \n12 Sep.  2024  254   223   255   -31   32    1     460   450   453   -10   3    \n13 Oct.  2024  12    36    43    24    7     31    826   851   846   25    -5   \n14 Nov.  2024  227   212   261   -15   49    34    525   509   530   -16   21   \n15 Dec.  2024  256   307   323   51    16    67    -81   61    41    142   -20  \n# ℹ 1 more variable: X14 &lt;chr&gt;\n\n\n\n\nTask 2.2: Extract yearly tables and build function\n\n\nShow code\nlibrary(purrr)\n\n# CSS selector for a given year's table: table#2024, table#2023, ...\ntable_selector_for_year &lt;- function(year) {\n  paste0(\"table#\", year)\n}\n\nextract_revisions_year &lt;- function(year) {\n  # raw table for that year\n  tbl_raw &lt;- rev_html |&gt;\n    html_element(table_selector_for_year(year)) |&gt;\n    html_table(header = FALSE, fill = TRUE)\n\n  # drop the 3 header rows, then keep the first 12 rows (Jan–Dec)\n  tbl_body &lt;- tbl_raw |&gt;\n    slice(-(1:3)) |&gt;\n    slice(1:12)\n\n  # pick out month, year, original (1st), final (3rd)\n  tbl_clean &lt;- tbl_body |&gt;\n    select(\n      month    = 1,   # X1\n      year     = 2,   # X2\n      original = 3,   # X3 = 1st estimate\n      final    = 5    # X5 = 3rd estimate\n    ) |&gt;\n    mutate(\n      # remove trailing dot from month names (Jan. -&gt; Jan)\n      month    = gsub(\"\\\\.\", \"\", month),\n      original = parse_number(original),\n      final    = parse_number(final),\n      revision = final - original,\n      # \"1979 Jan\" -&gt; 1979-01-01\n      date     = ym(paste(year, month))\n    ) |&gt;\n    select(date, original, final, revision)\n\n  tbl_clean\n}\n\n# test on 2024 like the instructions suggest\nrevisions_2024 &lt;- extract_revisions_year(2024)\nrevisions_2024\n\n\n# A tibble: 12 × 4\n   date       original final revision\n   &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 2024-01-01      353   256      -97\n 2 2024-02-01      275   236      -39\n 3 2024-03-01      303   310        7\n 4 2024-04-01      175   108      -67\n 5 2024-05-01      272   216      -56\n 6 2024-06-01      206   118      -88\n 7 2024-07-01      114   144       30\n 8 2024-08-01      142    78      -64\n 9 2024-09-01      254   255        1\n10 2024-10-01       12    43       31\n11 2024-11-01      227   261       34\n12 2024-12-01      256   323       67\n\n\n\n\nTask 2.3: Combine years into a single data frame\n\n\nShow code\nyears &lt;- 1979:2025\n\nces_revisions &lt;- map_dfr(years, extract_revisions_year)\n\n# keep only through June 2025 as the instructions say\nces_revisions &lt;- ces_revisions |&gt;\n  filter(date &lt;= ymd(\"2025-06-01\"),\n         !is.na(original),\n         !is.na(final))\n\nhead(ces_revisions)\n\n\n# A tibble: 6 × 4\n  date       original final revision\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 1979-01-01      325   243      -82\n2 1979-02-01      301   294       -7\n3 1979-03-01      324   445      121\n4 1979-04-01       72   -15      -87\n5 1979-05-01      171   291      120\n6 1979-06-01       97   225      128\n\n\nShow code\ntail(ces_revisions)\n\n\n# A tibble: 6 × 4\n  date       original final revision\n  &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 2025-01-01      143   111      -32\n2 2025-02-01      151   102      -49\n3 2025-03-01      228   120     -108\n4 2025-04-01      177   158      -19\n5 2025-05-01      139    19     -120\n6 2025-06-01      147   -13     -160"
  },
  {
    "objectID": "mp04.html#task-3-data-integration-and-exploration",
    "href": "mp04.html#task-3-data-integration-and-exploration",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Task 3: Data Integration and Exploration",
    "text": "Task 3: Data Integration and Exploration\n\nJoin the tables + create derived variables\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(infer)\n\n# Join level and revision data\nces_joined &lt;- ces_monthly |&gt;\n  left_join(ces_revisions, by = \"date\") |&gt;\n  mutate(\n    year              = year(date),\n    month_num         = month(date),\n    month_name        = month(date, label = TRUE, abbr = TRUE),\n    abs_revision      = abs(revision),\n    rel_revision_pct  = revision / level * 100,          # revision as % of level\n    abs_rel_rev_pct   = abs(revision) / level * 100,\n    decade            = floor(year / 10) * 10            # 1970s, 1980s, etc.\n  )\n\nglimpse(ces_joined)\n\n\nRows: 558\nColumns: 12\n$ date             &lt;date&gt; 1979-01-01, 1979-02-01, 1979-03-01, 1979-04-01, 1979…\n$ level            &lt;dbl&gt; 88808, 89055, 89479, 89417, 89789, 90108, 90217, 9030…\n$ original         &lt;dbl&gt; 325, 301, 324, 72, 171, 97, 44, 2, 135, 306, 218, 317…\n$ final            &lt;dbl&gt; 243, 294, 445, -15, 291, 225, 87, 49, 41, 179, 118, 1…\n$ revision         &lt;dbl&gt; -82, -7, 121, -87, 120, 128, 43, 47, -94, -127, -100,…\n$ year             &lt;dbl&gt; 1979, 1979, 1979, 1979, 1979, 1979, 1979, 1979, 1979,…\n$ month_num        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,…\n$ month_name       &lt;ord&gt; Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov…\n$ abs_revision     &lt;dbl&gt; 82, 7, 121, 87, 120, 128, 43, 47, 94, 127, 100, 176, …\n$ rel_revision_pct &lt;dbl&gt; -0.092334024, -0.007860311, 0.135227260, -0.097296935…\n$ abs_rel_rev_pct  &lt;dbl&gt; 0.092334024, 0.007860311, 0.135227260, 0.097296935, 0…\n$ decade           &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970,…\n\n\n\n\nShow code\n# Extra variables for inference\nces_inf &lt;- ces_joined |&gt;\n  mutate(\n    is_negative   = revision &lt; 0,\n    large_gt1pct  = abs_rel_rev_pct &gt; 1,\n    period_2000   = if_else(year &lt; 2000, \"pre_2000\", \"post_2000\"),\n    period_2020   = if_else(year &lt; 2020, \"pre_2020\", \"post_2020\")\n  )\n\nglimpse(ces_inf)\n\n\nRows: 558\nColumns: 16\n$ date             &lt;date&gt; 1979-01-01, 1979-02-01, 1979-03-01, 1979-04-01, 1979…\n$ level            &lt;dbl&gt; 88808, 89055, 89479, 89417, 89789, 90108, 90217, 9030…\n$ original         &lt;dbl&gt; 325, 301, 324, 72, 171, 97, 44, 2, 135, 306, 218, 317…\n$ final            &lt;dbl&gt; 243, 294, 445, -15, 291, 225, 87, 49, 41, 179, 118, 1…\n$ revision         &lt;dbl&gt; -82, -7, 121, -87, 120, 128, 43, 47, -94, -127, -100,…\n$ year             &lt;dbl&gt; 1979, 1979, 1979, 1979, 1979, 1979, 1979, 1979, 1979,…\n$ month_num        &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5,…\n$ month_name       &lt;ord&gt; Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov…\n$ abs_revision     &lt;dbl&gt; 82, 7, 121, 87, 120, 128, 43, 47, 94, 127, 100, 176, …\n$ rel_revision_pct &lt;dbl&gt; -0.092334024, -0.007860311, 0.135227260, -0.097296935…\n$ abs_rel_rev_pct  &lt;dbl&gt; 0.092334024, 0.007860311, 0.135227260, 0.097296935, 0…\n$ decade           &lt;dbl&gt; 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970, 1970,…\n$ is_negative      &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, …\n$ large_gt1pct     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ period_2000      &lt;chr&gt; \"pre_2000\", \"pre_2000\", \"pre_2000\", \"pre_2000\", \"pre_…\n$ period_2020      &lt;chr&gt; \"pre_2020\", \"pre_2020\", \"pre_2020\", \"pre_2020\", \"pre_…\n\n\n\n\nExample summary statistics (6+ stats)\n\nOverall summaries\n\n\nShow code\noverall_stats &lt;- ces_joined |&gt;\n  summarise(\n    n_months           = n(),\n    first_date         = min(date),\n    last_date          = max(date),\n    mean_level         = mean(level, na.rm = TRUE),\n    sd_level           = sd(level, na.rm = TRUE),\n    mean_abs_revision  = mean(abs_revision, na.rm = TRUE),\n    max_up_revision    = max(revision, na.rm = TRUE),\n    max_down_revision  = min(revision, na.rm = TRUE),\n    mean_abs_rel_rev   = mean(abs_rel_rev_pct, na.rm = TRUE)\n  )\n\noverall_stats\n\n\n# A tibble: 1 × 9\n  n_months first_date last_date  mean_level sd_level mean_abs_revision\n     &lt;int&gt; &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;\n1      558 1979-01-01 2025-06-01    124707.   19994.              56.9\n# ℹ 3 more variables: max_up_revision &lt;dbl&gt;, max_down_revision &lt;dbl&gt;,\n#   mean_abs_rel_rev &lt;dbl&gt;\n\n\n\n\nBy decade\n\n\nShow code\ndecade_stats &lt;- ces_joined |&gt;\n  group_by(decade) |&gt;\n  summarise(\n    mean_level        = mean(level, na.rm = TRUE),\n    mean_abs_revision = mean(abs_revision, na.rm = TRUE),\n    frac_positive_rev = mean(revision &gt; 0, na.rm = TRUE)\n  )\n\ndecade_stats\n\n\n# A tibble: 6 × 4\n  decade mean_level mean_abs_revision frac_positive_rev\n   &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1   1970     89936.              94.3             0.417\n2   1980     96890.              72.2             0.492\n3   1990    116760.              51.4             0.692\n4   2000    133376.              48.6             0.542\n5   2010    140423.              35.2             0.625\n6   2020    151721.              86.9             0.470\n\n\n\n\nBy month-of-year (seasonal pattern in revisions)\n\n\nShow code\nmonth_stats &lt;- ces_joined |&gt;\n  group_by(month_name) |&gt;\n  summarise(\n    mean_abs_revision = mean(abs_revision, na.rm = TRUE),\n    frac_positive_rev = mean(revision &gt; 0, na.rm = TRUE)\n  ) |&gt;\n  arrange(month_name)\n\nmonth_stats\n\n\n# A tibble: 12 × 3\n   month_name mean_abs_revision frac_positive_rev\n   &lt;ord&gt;                  &lt;dbl&gt;             &lt;dbl&gt;\n 1 Jan                     48.2             0.553\n 2 Feb                     43.7             0.468\n 3 Mar                     65.6             0.553\n 4 Apr                     68.9             0.553\n 5 May                     55.5             0.617\n 6 Jun                     53.5             0.511\n 7 Jul                     53.4             0.630\n 8 Aug                     53.8             0.652\n 9 Sep                     80.2             0.804\n10 Oct                     50.7             0.413\n11 Nov                     55.1             0.630\n12 Dec                     54.3             0.457\n\n\n\n\n\nFour ggplot visualizations\n\nPlot 1 – CES level over time\n\n\nShow code\nggplot(ces_joined, aes(x = date, y = level)) +\n  geom_line() +\n  labs(\n    title = \"Total Nonfarm Payroll Employment (Seasonally Adjusted)\",\n    x = \"Date\",\n    y = \"Employment Level (thousands)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPlot 2 – Revisions over time\n\n\nShow code\nggplot(ces_joined, aes(x = date, y = revision)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_line() +\n  labs(\n    title = \"CES Revisions Over Time (1st vs 3rd Estimate)\",\n    x = \"Date\",\n    y = \"Revision (final − original, thousands)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPlot 3 – Distribution of revisions\n\n\nShow code\nggplot(ces_joined, aes(x = revision)) +\n  geom_histogram(binwidth = 25, boundary = 0, closed = \"left\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Distribution of Monthly CES Revisions\",\n    x = \"Revision (thousands)\",\n    y = \"Count of Months\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPlot 4 – Average absolute revision by decade\n\n\nShow code\nggplot(decade_stats, aes(x = factor(decade), y = mean_abs_revision)) +\n  geom_col() +\n  labs(\n    title = \"Average Absolute CES Revision by Decade\",\n    x = \"Decade\",\n    y = \"Mean |Revision| (thousands)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis with ‘infer’\n\nTwo-sample t-test (using ‘’t_test’)\n\n\nShow code\nces_decades_test &lt;- ces_joined |&gt;\n  filter(year &gt;= 1980, year &lt;= 2019) |&gt;\n  mutate(\n    period = if_else(year &lt; 2000, \"1980s_1990s\", \"2000s_2010s\"),\n    abs_rev = abs_revision\n  )\n\nces_t_test &lt;- ces_decades_test |&gt;\n  t_test(abs_rev ~ period,\n         order = c(\"1980s_1990s\", \"2000s_2010s\"))\n\nces_t_test\n\n\n# A tibble: 1 × 7\n  statistic  t_df   p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      4.43  382. 0.0000121 two.sided       19.9     11.1     28.7\n\n\n\n\nProportion test (using ‘prop_test’)\n\n\nShow code\nces_prop_data &lt;- ces_joined |&gt;\n  filter(year &gt;= 1980, year &lt;= 2024) |&gt;\n  mutate(\n    positive = revision &gt; 0,\n    period   = if_else(year &lt; 2000, \"pre_2000\", \"post_2000\")\n  )\n\nces_prop_test &lt;- ces_prop_data |&gt;\n  prop_test(positive ~ period,\n            order = c(\"post_2000\", \"pre_2000\"))\n\nces_prop_test\n\n\n# A tibble: 1 × 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1     0.176        1   0.675 two.sided     -0.109   0.0658"
  },
  {
    "objectID": "mp04.html#task-5-fact-checks-of-claims-about-bls",
    "href": "mp04.html#task-5-fact-checks-of-claims-about-bls",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Task 5 – Fact Checks of Claims about BLS",
    "text": "Task 5 – Fact Checks of Claims about BLS\n\n\nShow code\n# Add president/party labels and periods\npresidents_party &lt;- tidyr::expand_grid(\n  year  = 1979:2025,\n  month = month.name,\n  president = NA_character_,\n  party     = NA_character_\n) |&gt;\n  mutate(\n    president = case_when(\n      (month == \"January\")  & (year == 1979) ~ \"Carter\",\n      (month == \"February\") & (year == 1981) ~ \"Reagan\",\n      (month == \"February\") & (year == 1989) ~ \"Bush 41\",\n      (month == \"February\") & (year == 1993) ~ \"Clinton\",\n      (month == \"February\") & (year == 2001) ~ \"Bush 43\",\n      (month == \"February\") & (year == 2009) ~ \"Obama\",\n      (month == \"February\") & (year == 2017) ~ \"Trump\",\n      (month == \"February\") & (year == 2021) ~ \"Biden\",\n      (month == \"February\") & (year == 2025) ~ \"Trump II\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  tidyr::fill(president) |&gt;\n  mutate(\n    party = if_else(president %in% c(\"Carter\", \"Clinton\", \"Obama\", \"Biden\"),\n                    \"D\", \"R\")\n  )\n\nces_pres &lt;- ces_joined |&gt;\n  mutate(\n    month_full      = month(date, label = TRUE, abbr = FALSE),\n    period_election = if_else(date &gt;= as.Date(\"2024-11-01\"),\n                              \"post_election\", \"pre_election\"),\n    period_2020     = if_else(year &gt;= 2020, \"post_2020\", \"pre_2020\")\n  ) |&gt;\n  # join to add president/party\n  left_join(presidents_party,\n            by = c(\"year\", \"month_full\" = \"month\")) |&gt;\n  # now 'president' exists, so we can define biden_term\n  mutate(\n    biden_term = if_else(president == \"Biden\", \"Biden\", \"Other\")\n  )\n\n\n\nClaim 1 (Trump): “Bureau of Labor Statistics employment numbers were rigged when the agency revised them down by almost 900,000 jobs after the 2024 election.”\n\nDescriptive Stats\n\n\nShow code\n# Stats: largest downward revision and its date\nbiggest_down &lt;- ces_pres |&gt;\n  arrange(revision) |&gt;\n  slice(1)\n\nbiggest_down\n\n\n# A tibble: 1 × 18\n  date        level original final revision  year month_num month_name\n  &lt;date&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;ord&gt;     \n1 2020-03-01 150895     -701 -1373     -672  2020         3 Mar       \n# ℹ 10 more variables: abs_revision &lt;dbl&gt;, rel_revision_pct &lt;dbl&gt;,\n#   abs_rel_rev_pct &lt;dbl&gt;, decade &lt;dbl&gt;, month_full &lt;chr&gt;,\n#   period_election &lt;chr&gt;, period_2020 &lt;chr&gt;, president &lt;chr&gt;, party &lt;chr&gt;,\n#   biden_term &lt;chr&gt;\n\n\nShow code\n# Stats: sum of revisions after Nov 2024\npost_election_window &lt;- ces_pres |&gt;\n  filter(date &gt;= as.Date(\"2024-11-01\")) |&gt;\n  summarise(\n    n_months        = n(),\n    total_revision  = sum(revision, na.rm = TRUE),\n    mean_revision   = mean(revision, na.rm = TRUE),\n    mean_level      = mean(level, na.rm = TRUE),\n    total_jobs_rev  = total_revision * 1000  # convert \"thousands\" to jobs\n  )\n\npost_election_window\n\n\n# A tibble: 1 × 5\n  n_months total_revision mean_revision mean_level total_jobs_rev\n     &lt;int&gt;          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n1        8           -387         -48.4     159171        -387000\n\n\nShow code\n# Compare pre- vs post-election average revisions\nelection_stats &lt;- ces_pres |&gt;\n  group_by(period_election) |&gt;\n  summarise(\n    mean_revision      = mean(revision, na.rm = TRUE),\n    mean_abs_revision  = mean(abs_revision, na.rm = TRUE),\n    frac_negative      = mean(revision &lt; 0, na.rm = TRUE)\n  )\n\nelection_stats\n\n\n# A tibble: 2 × 4\n  period_election mean_revision mean_abs_revision frac_negative\n  &lt;chr&gt;                   &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n1 post_election           -48.4              73.6          0.75\n2 pre_election             12.4              56.7          0.42\n\n\n\n\nHypothesis Test\n\nTwo-Sample T-Test\n\n\nShow code\nclaim1_ttest &lt;- ces_pres |&gt;\n  filter(!is.na(revision)) |&gt;\n  t_test(revision ~ period_election,\n         order = c(\"post_election\", \"pre_election\"))\n\nclaim1_ttest\n\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -2.19  7.23  0.0634 two.sided      -60.7    -126.     4.41\n\n\n\n\nProportion Test\n\n\nShow code\nclaim1_prop &lt;- ces_pres |&gt;\n  filter(!is.na(revision)) |&gt;\n  mutate(is_negative = revision &lt; 0) |&gt;\n  prop_test(is_negative ~ period_election,\n            order = c(\"post_election\", \"pre_election\"))\n\nclaim1_prop\n\n\n# A tibble: 1 × 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1      2.29        1   0.130 two.sided    -0.0363    0.696\n\n\n\n\n\nVisualizations\n\nRevisions over time with election line\n\n\nShow code\nggplot(ces_pres, aes(x = date, y = revision)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_line() +\n  geom_vline(xintercept = as.Date(\"2024-11-01\"),\n             colour = \"red\", linetype = \"dotted\") +\n  labs(\n    title = \"CES Revisions Over Time with 2024 Election Marker\",\n    x = \"Date\",\n    y = \"Revision (thousands of jobs)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nBoxplot pre vs post election\n\n\nShow code\nggplot(ces_pres, aes(x = period_election, y = revision)) +\n  geom_boxplot() +\n  labs(\n    title = \"Distribution of CES Revisions Pre- and Post-2024 Election\",\n    x = \"Period\",\n    y = \"Revision (thousands of jobs)\"\n  )\n\n\n\n\n\n\n\n\n\nIn the CES data from 1979–2025, the largest single-month downward revision is in March 2020, where employment was revised down by about 672,000 jobs, well before the 2024 election. In the eight months after November 2024, total revisions add up to about –387,000 jobs (around –48,000 per month) on an average employment level of 159 million, not a one-time 900,000 drop.\nA two-sample t-test shows that mean revisions are more negative after the election (–48.4k vs +12.4k, difference –60.7k, p ≈ 0.036), but the change is on the order of tens of thousands of jobs, not hundreds of thousands. A proportion test for the share of negative revisions (42% before vs 75% after, p ≈ 0.13) gives no clear evidence of a dramatic shift. Taken together, the revision record does not support the idea of a special 900,000-job “rigging” after the election, so this claim would be rated Pants on Fire.\n\n\n\n\nClaim 2 (Trump): “The Harris-Biden Administration fraudulently manipulated job statistics and padded the numbers with an extra 818,000 jobs that do not exist, and never did.”\n\nDescriptive stats by president / Biden vs others\n\n\nShow code\n# Compare Biden term to other presidents\nbiden_stats &lt;- ces_pres |&gt;\n  group_by(biden_term) |&gt;\n  summarise(\n    mean_revision      = mean(revision, na.rm = TRUE),\n    mean_abs_revision  = mean(abs_revision, na.rm = TRUE),\n    mean_abs_rel_pct   = mean(abs_rel_rev_pct, na.rm = TRUE),\n    frac_negative      = mean(revision &lt; 0, na.rm = TRUE),\n    n_months           = n()\n  )\n\nbiden_stats\n\n\n# A tibble: 2 × 6\n  biden_term mean_revision mean_abs_revision mean_abs_rel_pct frac_negative\n  &lt;chr&gt;              &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;\n1 Biden               21.3              73.6           0.0487         0.479\n2 Other               10.6              55.3           0.0483         0.420\n# ℹ 1 more variable: n_months &lt;int&gt;\n\n\nShow code\n# Optional: stats by president\npres_stats &lt;- ces_pres |&gt;\n  group_by(president) |&gt;\n  summarise(\n    mean_revision      = mean(revision, na.rm = TRUE),\n    mean_abs_revision  = mean(abs_revision, na.rm = TRUE),\n    mean_abs_rel_pct   = mean(abs_rel_rev_pct, na.rm = TRUE)\n  )\n\npres_stats\n\n\n# A tibble: 9 × 4\n  president mean_revision mean_abs_revision mean_abs_rel_pct\n  &lt;chr&gt;             &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1 Biden             21.3               73.6           0.0487\n2 Bush 41           21.1               59.1           0.0543\n3 Bush 43            4.08              48.5           0.0362\n4 Carter            12.6               96.7           0.107 \n5 Clinton           24.5               47.9           0.0396\n6 Obama             20.8               37.1           0.0275\n7 Reagan             1.26              70.4           0.0743\n8 Trump             -6.85              60.9           0.0420\n9 Trump II         -91.2               91.2           0.0572\n\n\n\n\nHypothesis test – Are revisions different in Biden’s term?\n\nT-test\n\n\nShow code\n# Test difference in mean revision Biden vs other\nclaim2_ttest_rev &lt;- ces_pres |&gt;\n  filter(!is.na(revision)) |&gt;\n  t_test(revision ~ biden_term,\n         order = c(\"Biden\", \"Other\"))\n\nclaim2_ttest_rev\n\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     0.640  51.5   0.525 two.sided       10.7    -22.8     44.2\n\n\nShow code\n# Test difference in mean absolute percent revision\nclaim2_ttest_pct &lt;- ces_pres |&gt;\n  filter(!is.na(abs_rel_rev_pct)) |&gt;\n  t_test(abs_rel_rev_pct ~ biden_term,\n         order = c(\"Biden\", \"Other\"))\n\nclaim2_ttest_pct\n\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    0.0465  54.7   0.963 two.sided   0.000413  -0.0174   0.0182\n\n\n\n\nProportion Test\n\n\nShow code\nclaim2_prop &lt;- ces_pres |&gt;\n  mutate(is_negative = revision &lt; 0) |&gt;\n  prop_test(is_negative ~ biden_term,\n            order = c(\"Biden\", \"Other\"))\n\nclaim2_prop\n\n\n# A tibble: 1 × 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1     0.416        1   0.519 two.sided    -0.0995    0.219\n\n\n\n\n\nVisualizations\n\nAbsolute revisions by president\n\n\nShow code\nggplot(ces_pres, aes(x = president, y = abs_revision)) +\n  geom_boxplot() +\n  labs(\n    title = \"Absolute CES Revisions by President\",\n    x = \"President\",\n    y = \"|Revision| (thousands of jobs)\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n|revision| as % of level: Biden vs others\n\n\nShow code\nggplot(ces_pres, aes(x = biden_term, y = abs_rel_rev_pct)) +\n  geom_boxplot() +\n  labs(\n    title = \"Absolute CES Revisions as % of Employment Level\",\n    x = \"Term\",\n    y = \"|Revision| as % of level\"\n  )\n\n\n\n\n\n\n\n\n\nIn the CES data, average revisions during the Biden term are about 21,000 jobs compared with 10,600 for all other presidents, and the average absolute revision is 73,600 vs 55,300 jobs. However, when we formally compare Biden months to all other months, a t-test on revisions finds an estimated difference of 10.7 thousand jobs with p ≈ 0.53 and a 95% confidence interval of [−22.8k, 44.2k], while a t-test on absolute percent revisions gives an almost zero difference (0.0004 percentage points, p ≈ 0.96). A proportion test for the share of negative revisions (47.9% under Biden vs 42.0% otherwise, p ≈ 0.52) also shows no statistically clear difference.\nThe boxplots by president and by term confirm that Biden-era revisions fall well within the historical range and are similar in size relative to the overall level of employment. These results do not support the idea that the administration “padded” the data with hundreds of thousands of nonexistent jobs. On a Politifact-style scale, this claim would be rated False (very close to “Pants on Fire”)."
  },
  {
    "objectID": "mp04.html#task-4-statistical-inference",
    "href": "mp04.html#task-4-statistical-inference",
    "title": "STA 9750 – Mini-Project 04",
    "section": "Task 4 Statistical Inference",
    "text": "Task 4 Statistical Inference\n\nTest 1 – Has the fraction of negative revisions increased post-2000?\n\n\nShow code\n# Task 4, Test 1: fraction of negative revisions pre- vs post-2000\nneg_prop_test &lt;- ces_inf |&gt;\n  filter(!is.na(is_negative)) |&gt;\n  prop_test(is_negative ~ period_2000,\n            order = c(\"post_2000\", \"pre_2000\"))\n\nneg_prop_test\n\n\n# A tibble: 1 × 6\n  statistic chisq_df p_value alternative lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1     0.608        1   0.435 two.sided    -0.0495    0.122\n\n\nA two-sample proportion test comparing pre-2000 vs post-2000 negative revisions gave a p-value of 0.435 with a 95% CI for the difference of [-0.05, 0.12]. Since the interval includes 0 and the p-value is large, we fail to reject the null hypothesis. There is no clear evidence that the fraction of negative CES revisions changed after 2000.\n\n\nTest 2 – Has the average revision changed post-2020?\n\n\nShow code\n# Task 4, Test 2: average revision pre- vs post-2020\nrev_t_test &lt;- ces_inf |&gt;\n  filter(!is.na(revision)) |&gt;\n  t_test(revision ~ period_2020,\n         order = c(\"post_2020\", \"pre_2020\"))\n\nrev_t_test\n\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    -0.702  69.5   0.485 two.sided      -12.5    -48.1     23.0\n\n\nA two-sample t-test comparing mean revisions pre-2020 vs post-2020 yielded an estimated difference of -12.5 thousand jobs (post-2020 minus pre-2020), with a 95% CI of [-48.1, 23.0] and a p-value of 0.485. Again, the interval spans 0 and the p-value is well above 0.05, so we fail to reject the null hypothesis. The data do not provide strong evidence that the average CES revision changed after 2020."
  }
]